{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 413
    },
    "id": "CkuKoMdNxJI1",
    "outputId": "2acb6aab-55d7-48e8-fcfd-696b78b1c13a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import display\n",
    "\n",
    "import torch\n",
    "\n",
    "# تحميل البيانات\n",
    "cols_dict = {\n",
    "    'ratings': ['user_id', 'movie_id', 'rating', 'timestamp'],\n",
    "    'users': ['user_id', 'gender', 'age', 'occupation', 'zip_code'],\n",
    "    'items': ['movie_id', 'title', 'genre']\n",
    "}\n",
    "dir = 'C:/Users/lenovo/Desktop/LinUCB-HybridRecommender/data/1m/'\n",
    "\n",
    "ratings_data = pd.read_csv(dir + 'ratings.dat', sep='::', names=cols_dict['ratings'], engine='python')\n",
    "users_data = pd.read_csv(dir + 'users.dat', sep='::', names=cols_dict['users'], engine='python')\n",
    "items_data = pd.read_csv(dir + 'movies.dat', sep='::', names=cols_dict['items'], encoding='latin-1', engine='python')\n",
    "\n",
    "display(ratings_data.head(3), users_data.head(3), items_data.head(3))\n",
    "print(ratings_data.shape, users_data.shape, items_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import torch\n",
    "\n",
    "# # تعريف هيكل البيانات\n",
    "# cols_dict = {\n",
    "#     'ratings': ['user_id', 'movie_id', 'rating', 'timestamp'],\n",
    "#     'users': ['user_id', 'gender', 'age', 'occupation', 'zip_code'],\n",
    "#     'items': ['movie_id', 'title', 'genre']\n",
    "# }\n",
    "\n",
    "# # المسار إلى ملفات البيانات 100K\n",
    "# dir = 'C:/Users/lenovo/Desktop/LinUCB-HybridRecommender/data/100k/'  # تغيير المسار هنا\n",
    "\n",
    "# # تحميل البيانات مع معالجة الأخطاء المحتملة\n",
    "# try:\n",
    "#     ratings_data = pd.read_csv(dir + 'u.data', sep='\\t', names=cols_dict['ratings'])\n",
    "#     users_data = pd.read_csv(dir + 'u.user', sep='|', names=cols_dict['users'])\n",
    "#     items_data = pd.read_csv(dir + 'u.item', sep='|', names=cols_dict['items'] + ['other_details'], encoding='latin-1')\n",
    "    \n",
    "#     # تنظيف بيانات الأفلام (قد تختلف حسب هيكل ملف u.item)\n",
    "#     items_data = items_data[cols_dict['items']]  # الاحتفاظ بالأعمدة المطلوبة فقط\n",
    "    \n",
    "#     print(\"تم تحميل البيانات بنجاح!\")\n",
    "#     print(f\"أبعاد البيانات: {ratings_data.shape} (تقييمات), {users_data.shape} (مستخدمين), {items_data.shape} (أفلام)\")\n",
    "    \n",
    "# except Exception as e:\n",
    "#     print(f\"حدث خطأ أثناء تحميل البيانات: {str(e)}\")\n",
    "#     # تحميل بيانات مثاليه في حالة الخطأ (اختياري)\n",
    "#     # ratings_data, users_data, items_data = load_sample_data()\n",
    "\n",
    "# # عرض عينة من البيانات\n",
    "# display(ratings_data.head(3))\n",
    "# display(users_data.head(3))\n",
    "# display(items_data.head(3))\n",
    "\n",
    "# # معلومات إضافية عن البيانات\n",
    "# print(\"\\nمعلومات إضافية:\")\n",
    "# print(f\"عدد المستخدمين الفريدين: {ratings_data['user_id'].nunique()}\")\n",
    "# print(f\"عدد الأفلام الفريدة: {ratings_data['movie_id'].nunique()}\")\n",
    "# print(f\"النطاق الزمني للتقييمات: من {pd.to_datetime(ratings_data['timestamp'], unit='s').min()} إلى {pd.to_datetime(ratings_data['timestamp'], unit='s').max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7LivXHwsx8Dp",
    "outputId": "b5f1ba25-4952-4084-86e8-0e892cda2158"
   },
   "outputs": [],
   "source": [
    "\n",
    "# نسخة احتياطية\n",
    "users_data_og = users_data.copy(deep=True)\n",
    "items_data_og = items_data.copy(deep=True)\n",
    "ratings_data_og = ratings_data.copy(deep=True)\n",
    "print(users_data.columns)\n",
    "print(items_data.columns)\n",
    "print(ratings_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "10zoKlg6ygc2"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Literal, Union\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import ndcg_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class Utils:\n",
    "    @staticmethod\n",
    "    def extract_year(items_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        '''\n",
    "        Extracts:\n",
    "            - Year from title\n",
    "\n",
    "        returns: Dataframes with extracted features\n",
    "        '''\n",
    "        # Extract year from title\n",
    "        items_df['year'] = items_df['title'].str.extract(r'\\((\\d{4})\\)').astype(int)\n",
    "\n",
    "        return items_df\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_category_avg_ratings(users_df: pd.DataFrame, items_df: pd.DataFrame, ratings_df: pd.DataFrame, k=0.6) -> pd.DataFrame:\n",
    "        '''\n",
    "        Extracts penalized average ratings for each category for each user using exponential decay penalty.\n",
    "\n",
    "        users_df: DataFrame containing user information.\n",
    "        items_df: DataFrame containing item information with categories as binary columns.\n",
    "        ratings_df: DataFrame containing user-item interactions and ratings.\n",
    "        k: Control factor for penalty steepness. Default is 0.6.\n",
    "\n",
    "        Returns a DataFrame with users and their penalized average ratings per category.\n",
    "        '''\n",
    "        # Create a copy of users_df to store features\n",
    "        features_df = users_df.copy()\n",
    "\n",
    "        # Define exponential penalty function\n",
    "        def exp_penalty(n, k=0.6):\n",
    "            return 1 / np.exp(k * n)\n",
    "\n",
    "        # Iterate over each category in the items_df (excluding non-categorical columns)\n",
    "        for category in items_df.columns[2:]:\n",
    "            # Get item IDs in the current category\n",
    "            category_items = items_df[items_df[category] == 1]['movie_id']\n",
    "\n",
    "            # Filter ratings_df to include only ratings for items in the current category\n",
    "            category_ratings = ratings_df[ratings_df['movie_id'].isin(category_items)]\n",
    "\n",
    "            # Group by user_id and calculate the average rating and count of ratings for the current category\n",
    "            user_stats = category_ratings.groupby('user_id')['rating'].agg(['mean', 'count']).reset_index()\n",
    "            user_stats.columns = ['user_id', f'user_avg_rating_{category}', f'count_rating_{category}']\n",
    "\n",
    "            # Merge the user stats into features_df\n",
    "            features_df = pd.merge(features_df, user_stats, on='user_id', how='left')\n",
    "\n",
    "            # Apply exponential penalty and calculate the penalized average rating for each user\n",
    "            features_df[f'user_avg_rating_{category}'] = (\n",
    "                (1 - exp_penalty(features_df[f'count_rating_{category}'], k)) * features_df[f'user_avg_rating_{category}']\n",
    "            )\n",
    "\n",
    "            # Fill missing values with 0\n",
    "            features_df[f'user_avg_rating_{category}'] = features_df[f'user_avg_rating_{category}'].fillna(0)\n",
    "\n",
    "        # Select only the relevant columns\n",
    "        cols = features_df.columns[:32].tolist() + [col for col in features_df.columns if col.startswith('user_avg_rating_')]\n",
    "        result_df = features_df[cols]\n",
    "\n",
    "        return result_df\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_category_freq(users_df: pd.DataFrame, items_df: pd.DataFrame, ratings_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        # Copy users_df to avoid modifying the original dataframe\n",
    "        freq_df = users_df.copy()\n",
    "\n",
    "        # Get total interactions for each user\n",
    "        total_interactions = ratings_df.groupby('user_id').size().reset_index(name='total_interactions')\n",
    "        freq_df = pd.merge(freq_df, total_interactions, on='user_id', how='left').fillna(0)\n",
    "\n",
    "        # Iterate over each category in the items_df (assuming categories are from the 3rd column onward)\n",
    "        for category in items_df.columns[2:]:\n",
    "            # Get movie_ids that belong to the current category\n",
    "            movie_ids_in_category = items_df[items_df[category] == 1]['movie_id']\n",
    "\n",
    "            # Count interactions in the current category for each user\n",
    "            category_interactions = ratings_df[ratings_df['movie_id'].isin(movie_ids_in_category)].groupby('user_id').size().reset_index(name=f'{category}_count')\n",
    "\n",
    "            # Merge category_interactions with freq_df\n",
    "            freq_df = pd.merge(freq_df, category_interactions, on='user_id', how='left').fillna(0)\n",
    "\n",
    "            # Calculate frequency of interactions for the current category\n",
    "            freq_df[f'freq_{category}'] = freq_df[f'{category}_count'] / freq_df['total_interactions']\n",
    "\n",
    "            # Drop the intermediate category count column\n",
    "            freq_df.drop(columns=[f'{category}_count'], inplace=True)\n",
    "\n",
    "        return freq_df.fillna(0).drop(columns=['total_interactions'])\n",
    "\n",
    "    @staticmethod\n",
    "    def extend_users_items(users_df: pd.DataFrame, items_df: pd.DataFrame, ratings_df: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        '''\n",
    "        Extends users and items dataframes to match the ratings dataframe\n",
    "        '''\n",
    "        # Extend users dataframe\n",
    "        users_df = pd.merge(users_df, ratings_df[['user_id']], on='user_id', how='right')\n",
    "\n",
    "        # Extend items dataframe\n",
    "        items_df = pd.merge(items_df, ratings_df[['movie_id']], on='movie_id', how='right')\n",
    "\n",
    "        return users_df, items_df\n",
    "\n",
    "    @staticmethod\n",
    "    def multi_hot_encode(df: pd.DataFrame, col: str, delimiter='|') -> pd.DataFrame:\n",
    "        '''\n",
    "        Multi hot encodes columns in a dataframe\n",
    "        '''\n",
    "        df_ = df.copy(deep=True)\n",
    "\n",
    "        # Change Children's to Children to match the other genres\n",
    "        df_[col] = df_[col].str.replace(\"Children's\", 'Children') if col == 'genre' else df_[col]\n",
    "\n",
    "        # split genres\n",
    "        df_[col] = df_[col].str.split(delimiter)\n",
    "\n",
    "        # Create a pivot table\n",
    "        pivot_df = df_.explode(col).pivot_table(index='movie_id', columns=col, aggfunc='size', fill_value=0).reset_index()\n",
    "\n",
    "        # Merge the pivot table with the original DataFrame on 'movie_id'\n",
    "        result = pd.merge(df, pivot_df, on='movie_id', how='left')\n",
    "\n",
    "        return result.drop(columns=[col])\n",
    "\n",
    "    @staticmethod\n",
    "    def one_hot_encode(df: pd.DataFrame, cols: list[str]) -> pd.DataFrame:\n",
    "        '''\n",
    "        One hot encodes columns in a dataframe\n",
    "        '''\n",
    "        return pd.get_dummies(df, columns=cols) * 1\n",
    "\n",
    "    @staticmethod\n",
    "    def move_column(df: pd.DataFrame, col: list[str], pos: int) -> pd.DataFrame:\n",
    "        '''\n",
    "        Moves a column to a specific position in a DataFrame\n",
    "        '''\n",
    "        cols = df.columns.tolist()\n",
    "        for i in reversed(col):\n",
    "            cols.insert(pos, cols.pop(cols.index(i)))\n",
    "        return df[cols]\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess_user(user: dict, num_items: int, users: np.ndarray, weights: list[np.ndarray]=None, topk: int=3, verbose=False) -> tuple[torch.IntTensor, torch.FloatTensor, Union[list[np.ndarray], None], Union[np.ndarray, None]]:\n",
    "        '''\n",
    "        Preprocesses user data for model input\n",
    "        '''\n",
    "        if 'age' not in user or not user['age']:\n",
    "            user_ = users[user['id'] - 1]\n",
    "            user_ = np.insert(user_, 0, user['id'])\n",
    "            print(f\"User id: {user['id']} top {topk} genres: {np.array(genre)[np.argsort(user_[-18:])[-topk:][::-1]]}\") if verbose else None\n",
    "            user_ = np.tile(user_, (num_items, 1))\n",
    "            return torch.IntTensor(user_[:, 0]), torch.FloatTensor(user_[:, 1:]), None, np.array(genre)[np.argsort(user_[0, -18:])[-topk:][::-1]]\n",
    "\n",
    "        user_ = np.zeros(31, dtype=float)\n",
    "\n",
    "        user_[0] = user['id']\n",
    "\n",
    "        user_[1 if user['gender'] == 'M' else 2] = 1\n",
    "\n",
    "        user_[3 + occupation.index(user['occupation'])] = 1\n",
    "\n",
    "        # map age to bins\n",
    "        user['age'] = 1 if user['age'] < 18 else 18 if user['age'] < 25 else 25 if user['age'] < 35 else 35 if user['age'] < 45 else 45 if user['age'] < 56 else 56\n",
    "\n",
    "        user_[3 + len(occupation) + age.index(user['age'])] = 1\n",
    "\n",
    "        avg_ratings = np.zeros(len(genre), dtype=float) # 18 genres\n",
    "\n",
    "        for genre_ in user['genres']:\n",
    "            avg_ratings[genre.index(genre_)] = 1.0\n",
    "\n",
    "        user_ = np.concatenate((user_, avg_ratings))\n",
    "\n",
    "        # Get top 10 users ids of users with similar intrests (cosine similarity)\n",
    "        similar_users_ids = cosine_similarity(user_[1:].reshape(1, -1), users).argsort()[0][-10:]\n",
    "\n",
    "        # Get the mean embeddings of the top 10 similar users\n",
    "        mlp_weights = weights[0][similar_users_ids].mean(axis=0)\n",
    "        mf_weights = weights[1][similar_users_ids].mean(axis=0)\n",
    "\n",
    "        user_ = np.tile(user_, (num_items, 1))\n",
    "        return torch.IntTensor(user_[:, 0]), torch.FloatTensor(user_[:, 1:]), [mlp_weights, mf_weights], None\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess_items(items: pd.DataFrame) -> pd.DataFrame:\n",
    "        '''\n",
    "        Preprocesses items data for model input\n",
    "        '''\n",
    "        # multi hot encode genres\n",
    "        items_ = Utils.multi_hot_encode(items, 'genre')\n",
    "        items_ = Utils.extract_year(items_)\n",
    "        items_['year'] = items_['year'] / items_['year'].max()\n",
    "        items_ = items_.drop(['title'], axis=1)\n",
    "\n",
    "        return items_\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_missing_values(ratings: pd.DataFrame, items: pd.DataFrame) -> tuple[pd.DataFrame]:\n",
    "        '''\n",
    "        Removes rows with missing values from items and ratings dataframes\n",
    "        '''\n",
    "        # Get item ids with missing release dates\n",
    "        nan_item_ids = items[items[['release_date']].isna().any(axis=1)]['item_id']\n",
    "\n",
    "        # Remove movies with missing release dates\n",
    "        items.dropna(subset=['release_date'], inplace=True)\n",
    "\n",
    "        # remove ratings of missing items\n",
    "        ratings = ratings[~ratings['item_id'].isin(nan_item_ids)]\n",
    "\n",
    "        return ratings, items\n",
    "\n",
    "    @staticmethod\n",
    "    def negative_sampling(ratings: pd.DataFrame, items: pd.DataFrame, num_negatives: int) -> pd.DataFrame:\n",
    "        '''\n",
    "        Sample negative items for each user\n",
    "        '''\n",
    "        # All Movie ids\n",
    "        all_items = items['movie_id'].values\n",
    "\n",
    "        negative_samples = []\n",
    "        for user_id in ratings['user_id'].unique():\n",
    "            # Movie ids that the user has interacted with\n",
    "            pos_items = ratings[ratings['user_id'] == user_id]['movie_id'].values\n",
    "\n",
    "            # Movie ids that the user has not interacted with\n",
    "            unrated_items = np.setdiff1d(all_items, pos_items)\n",
    "\n",
    "            # Sample negative items\n",
    "            neg_items = np.random.choice(unrated_items, size=num_negatives, replace=False)\n",
    "\n",
    "            # Create negative samples\n",
    "            for item_id in neg_items:\n",
    "                negative_samples.append([user_id, item_id, 0])\n",
    "\n",
    "        negative_samples = pd.DataFrame(negative_samples, columns=['user_id', 'movie_id', 'rating'])\n",
    "\n",
    "        ratings['rating'] = [1] * ratings.shape[0]\n",
    "\n",
    "        return pd.concat([ratings, negative_samples], ignore_index=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def ndcg_hit_ratio(y_preds, X_test_users, y_true, k=10):  # تغيير هنا: قيمة افتراضية لـ k\n",
    "        '''\n",
    "        Compute NDCG and Hit Ratio\n",
    "        '''\n",
    "        if k is None:  # إضافة تحقق\n",
    "            k = 10\n",
    "\n",
    "        unique_users = np.unique(X_test_users, axis=0)\n",
    "        hits = 0\n",
    "        total_users = len(unique_users)\n",
    "\n",
    "        y_preds_padded = []\n",
    "        y_true_padded = []\n",
    "        for user in unique_users:\n",
    "            user_indices = np.where((X_test_users == user).all(axis=1))[0]\n",
    "            user_preds = y_preds[user_indices][:k].flatten()\n",
    "            user_true = y_true[user_indices][:k].flatten()\n",
    "\n",
    "            if np.any(user_true == 1):\n",
    "                hits += 1\n",
    "\n",
    "            if len(user_preds) < k:\n",
    "                user_preds = np.pad(user_preds, (0, k - len(user_preds)), mode='constant', constant_values=-1e10)\n",
    "            if len(user_true) < k:\n",
    "                user_true = np.pad(user_true, (0, k - len(user_true)), mode='constant', constant_values=0)\n",
    "\n",
    "            y_preds_padded.append(user_preds)\n",
    "            y_true_padded.append(user_true)\n",
    "\n",
    "        ndcg = ndcg_score(y_true_padded, y_preds_padded, k=k)\n",
    "        hit_ratio = hits / total_users\n",
    "        return ndcg, hit_ratio\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def pipeline(request: any, model: nn.Module, weights: list[np.ndarray], users: np.ndarray, movies: pd.DataFrame, movies_og: pd.DataFrame, ratings: pd.DataFrame, mode: str) -> tuple[list[dict], Union[np.ndarray, None]]:\n",
    "        '''\n",
    "        Pipeline for inference\n",
    "        '''\n",
    "        num_items = 300 # Number of items to retrieve\n",
    "        request = request if isinstance(request, dict) else request.model_dump()\n",
    "\n",
    "        # preprocess the old user\n",
    "        user_id, user, weights, top_n_genres = Utils.preprocess_user(\n",
    "                                        user=request,\n",
    "                                        num_items=num_items,\n",
    "                                        users=users,\n",
    "                                        weights=weights\n",
    "                                        )\n",
    "        user_id, user = user_id.to(model.device), user.to(model.device)\n",
    "\n",
    "        movies = Utils.retrieve(\n",
    "            movies=movies,\n",
    "            user=user.detach().cpu().numpy(),\n",
    "            num_genres=len(request['genres']) if request['genres'] else 3,\n",
    "            k=num_items,\n",
    "            random_state=0\n",
    "        )\n",
    "\n",
    "        movie_ids, movies = Utils.filter(\n",
    "            movies=movies,\n",
    "            ratings=ratings,\n",
    "            user_id=request['id']\n",
    "        )\n",
    "        movie_ids, movies = movie_ids.to(model.device), movies.to(model.device)\n",
    "\n",
    "        y_pred = model(\n",
    "            user_id[:len(movies)],\n",
    "            movie_ids,\n",
    "            user[:len(movies)],\n",
    "            movies,\n",
    "            weights\n",
    "        ).cpu().detach().numpy()\n",
    "\n",
    "        movies_retrieved = movies_og[movies_og['movie_id'].isin(movie_ids.cpu().numpy())].sort_values(by='movie_id', key=lambda x: pd.Categorical(x, categories=movie_ids.cpu().numpy(), ordered=True))\n",
    "\n",
    "        return Utils.order(y_pred, movies_retrieved, mode, top_k=request['top_k']).to_dict(orient='records'), top_n_genres\n",
    "\n",
    "    @staticmethod\n",
    "    def retrieve(movies: pd.DataFrame, user: np.ndarray, k: int, num_genres: int=3, random_state: int=42) -> pd.DataFrame:\n",
    "        '''\n",
    "        Retrieve top k movies based on genres based on this equation:\n",
    "        ```\n",
    "        num_movies_per_genre = k // (len(genres) + 1) # +1 for the most popular genre\n",
    "        ```\n",
    "\n",
    "        Example:\n",
    "        If k = 100 and genres = ['Action', 'Adventure', 'Animation'], then:\n",
    "        25 movies will be retrieved for each genre and 25 for the most popular genre.\n",
    "\n",
    "        movies: DataFrame containing movie information.\n",
    "        genres: List of genres to retrieve movies for.\n",
    "        k: Number of movies to retrieve.\n",
    "\n",
    "        Returns a DataFrame containing the top k movies based on the specified genres.\n",
    "        '''\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def filter(movies: pd.DataFrame, ratings: pd.DataFrame, user_id: int) -> tuple[torch.IntTensor, torch.FloatTensor]:\n",
    "        '''\n",
    "        Filter movies that the user has not interacted with, and remove duplicates\n",
    "        '''\n",
    "        # Get movie ids that the user has interacted with\n",
    "        user_movies = ratings[ratings['user_id'] == user_id]['movie_id'].values\n",
    "\n",
    "        # Filter movies that the user has not interacted with\n",
    "        movies = movies[~movies['movie_id'].isin(user_movies)]\n",
    "\n",
    "        # Remove duplicates\n",
    "        movies = movies.drop_duplicates(subset=['movie_id'])\n",
    "\n",
    "        return torch.IntTensor(movies['movie_id'].values), torch.FloatTensor(movies.drop(columns=['movie_id']).values)\n",
    "\n",
    "    @staticmethod\n",
    "    def order(y_pred: np.ndarray, movies: pd.DataFrame, mode: Literal['explicit', 'implicit'], top_k=10) -> list[dict]:\n",
    "        '''\n",
    "        Order the predictions\n",
    "        '''\n",
    "        col_name= 'predicted_rating' if mode == 'explicit' else 'predicted_score'\n",
    "        sorted_index = np.argsort(-y_pred, axis=0).reshape(-1).tolist()\n",
    "        y_pred = y_pred[sorted_index]\n",
    "        sorted_movies = movies.iloc[sorted_index]\n",
    "        sorted_movies = sorted_movies.copy()\n",
    "        sorted_movies[col_name] = y_pred if mode == 'implicit' else y_pred * 5\n",
    "        sorted_movies.reset_index(drop=True, inplace=True)\n",
    "        sorted_movies[col_name] = sorted_movies[col_name].apply(lambda x: round(x, 2))\n",
    "\n",
    "        return sorted_movies.head(top_k)\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_metrics(history: dict, title: str, figsize: tuple=(12, 4)) -> None:\n",
    "        '''\n",
    "        Plot the training and validation losses in one figure and the other metrics in another figure\n",
    "        '''\n",
    "        fig, ax = plt.subplots(1, 2, figsize=figsize)\n",
    "        ax[0].plot(history['loss'], label='Train Loss')\n",
    "        ax[0].plot(history['val_loss'], label='Validation Loss')\n",
    "        ax[0].set_title('Training and Validation Loss')\n",
    "        ax[0].set_xlabel('Epoch')\n",
    "        ax[0].set_ylabel('Loss')\n",
    "        ax[0].legend()\n",
    "\n",
    "        # Metrics plot\n",
    "        for metric, values in history.items():\n",
    "            if metric not in ['loss', 'val_loss']:\n",
    "                ax[1].plot(values, label=metric)\n",
    "\n",
    "        ax[1].set_title('Metrics')\n",
    "        ax[1].set_xlabel('Epoch')\n",
    "        ax[1].set_ylabel('Value')\n",
    "        ax[1].legend()\n",
    "        plt.suptitle(title)\n",
    "        plt.show()\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stopping to stop the training when the loss does not improve after a certain number of epochs (patience).\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=3, delta=0, verbose=False, path='checkpoint.pth') -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How many epochs to wait after the last time the validation loss improved.\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.verbose = verbose\n",
    "        self.path = path\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "\n",
    "    def __call__(self, val_loss, model: nn.Module) -> None:\n",
    "        '''\n",
    "        Call method\n",
    "        '''\n",
    "        score = -val_loss\n",
    "\n",
    "        if not self.best_score:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}') if self.verbose else None\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model: nn.Module) -> None:\n",
    "        '''\n",
    "        Save the model checkpoint\n",
    "        '''\n",
    "        print(f'Validation loss decreased ({self.best_loss:.6f} --> {val_loss:.6f}).  Saving model ...') if self.verbose else None\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.best_loss = val_loss\n",
    "\n",
    "age = [\n",
    "    1, 18, 25, 35, 45, 50, 56\n",
    "]\n",
    "\n",
    "occupation = [\n",
    "    'other', 'educator', 'artist', 'clerical', 'grad student',\n",
    "    'customer service', 'doctor', 'executive', 'farmer', 'homemaker',\n",
    "    'K-12 student', 'lawyer', 'programmer', 'retired', 'sales', 'scientist',\n",
    "    'self-employed', 'engineer', 'craftsman', 'unemployed', 'writer'\n",
    "]\n",
    "\n",
    "genre = [\n",
    "    'Action', 'Adventure', 'Animation', 'Children', 'Comedy', 'Crime',\n",
    "    'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery',\n",
    "    'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western'\n",
    "]\n",
    "\n",
    "cols_dict = {\n",
    "    'ratings': ['user_id', 'movie_id', 'rating', 'timestamp'],\n",
    "    'users': ['user_id', 'gender', 'age', 'occupation', 'zip_code'],\n",
    "    'items': ['movie_id', 'title', 'genre'],\n",
    "}\n",
    "\n",
    "css = \"\"\"\n",
    "    <style>\n",
    "        .card-container {\n",
    "            display: flex;\n",
    "            flex-direction: row;\n",
    "            justify-content: center;\n",
    "            align-items: start;\n",
    "            gap: 20px;\n",
    "            flex-wrap: wrap;\n",
    "            margin: 20px 0;\n",
    "        }\n",
    "\n",
    "        .card {\n",
    "            width: 100%;\n",
    "            max-width: 300px;\n",
    "            border: 1px solid #ddd;\n",
    "            border-radius: 8px;\n",
    "            padding: 16px;\n",
    "            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);\n",
    "            background-color: #eee;\n",
    "            transition: transform 0.2s ease-in-out;\n",
    "        }\n",
    "\n",
    "        .card:hover {\n",
    "            transform: scale(1.05);\n",
    "        }\n",
    "\n",
    "        .card-title {\n",
    "            font-size: 1.25em;\n",
    "            margin-bottom: 8px;\n",
    "            color: #333;\n",
    "        }\n",
    "\n",
    "        .card-text {\n",
    "            font-size: 1em;\n",
    "            margin-bottom: 8px;\n",
    "            color: #555;\n",
    "        }\n",
    "\n",
    "        .footer {\n",
    "        position: fixed;\n",
    "        left: 0;\n",
    "        bottom: 0;\n",
    "        width: 100%;\n",
    "        background-color: rgb(45, 38, 48);\n",
    "        color: #fff;\n",
    "        text-align: center;\n",
    "        padding: 10px;\n",
    "    }\n",
    "    </style>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SeRfWRIpCZ6X"
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from enum import Enum\n",
    "\n",
    "class GenreEnum(str, Enum):\n",
    "    Action = \"Action\"\n",
    "    Adventure = \"Adventure\"\n",
    "    Animation = \"Animation\"\n",
    "    Childrens = \"Children\"\n",
    "    Comedy = \"Comedy\"\n",
    "    Crime = \"Crime\"\n",
    "    Documentary = \"Documentary\"\n",
    "    Drama = \"Drama\"\n",
    "    Fantasy = \"Fantasy\"\n",
    "    FilmNoir = \"Film-Noir\"\n",
    "    Horror = \"Horror\"\n",
    "    Musical = \"Musical\"\n",
    "    Mystery = \"Mystery\"\n",
    "    Romance = \"Romance\"\n",
    "    SciFi = \"Sci-Fi\"\n",
    "    Thriller = \"Thriller\"\n",
    "    War = \"War\"\n",
    "    Western = \"Western\"\n",
    "\n",
    "class OccupationEnum(str, Enum):\n",
    "    other = \"other\"\n",
    "    educator = \"educator\"\n",
    "    artist = \"artist\"\n",
    "    clerical = \"clerical\"\n",
    "    grad_student = \"grad student\"\n",
    "    customer_service = \"customer service\"\n",
    "    doctor = \"doctor\"\n",
    "    executive = \"executive\"\n",
    "    farmer = \"farmer\"\n",
    "    homemaker = \"homemaker\"\n",
    "    K_12_student = \"K-12 student\"\n",
    "    lawyer = \"lawyer\"\n",
    "    programmer = \"programmer\"\n",
    "    retired = \"retired\"\n",
    "    sales = \"sales\"\n",
    "    scientist = \"scientist\"\n",
    "    self_employed = \"self-employed\"\n",
    "    engineer = \"engineer\"\n",
    "    craftsman = \"craftsman\"\n",
    "    unemployed = \"unemployed\"\n",
    "    writer = \"writer\"\n",
    "\n",
    "class Request(BaseModel):\n",
    "    top_k: int = Field(10, ge=1, le=20, description=\"Number of recommendations\")\n",
    "    id: int = Field(..., ge=1, description=\"User\\'s ID\")\n",
    "    age: int = Field(None, ge=1, le=99, description=\"User\\'s age\")\n",
    "    occupation: OccupationEnum = Field(None, description=\"User\\'s occupation\")\n",
    "    gender: str = Field(None, pattern=\"^(M|F)$\", description=\"User\\'s gender\")\n",
    "    genres: list[GenreEnum] = Field(None, min_items=3, max_items=5, description=\"User\\'s favorite genres\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_data, items_data, ratings_data = users_data_og, items_data_og, ratings_data_og"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 532
    },
    "id": "YCv6qP8yyEsL",
    "outputId": "74a607b1-6519-403b-b31d-ba16b55b7e72"
   },
   "outputs": [],
   "source": [
    "# One-hot encode categorical features\n",
    "users_data = Utils.one_hot_encode(users_data, ['occupation', 'gender', 'age'])\n",
    "\n",
    "# Multi-hot encode genres\n",
    "items_data = Utils.multi_hot_encode(items_data, 'genre')\n",
    "\n",
    "# Features Extraction\n",
    "users_data = Utils.extract_category_freq(users_data, items_data, ratings_data)\n",
    "items_data = Utils.extract_year(items_data)\n",
    "\n",
    "# Negative Sampling for implicit feedback\n",
    "ratings_data = Utils.negative_sampling(ratings_data, items_data, num_negatives=25) # Adds 20 * 6040 rows (num_negatives * num_users)\n",
    "\n",
    "# Move gender columns to the front\n",
    "users_data = Utils.move_column(users_data, ['gender_M', 'gender_F'], 0)\n",
    "\n",
    "# Extend Users and Items data to match the number of rows in the ratings data\n",
    "users_data, items_data = Utils.extend_users_items(users_data, items_data, ratings_data)\n",
    "\n",
    "# Normalize continuous features\n",
    "items_data['year'] = items_data['year'].astype(float) / items_data['year'].max()\n",
    "\n",
    "# Drop unnecessary columns\n",
    "ratings_data = ratings_data.drop(['timestamp'], axis=1) # Implicit Interactions\n",
    "users_data = users_data.drop(['user_id', 'zip_code'], axis=1) # Demographics\n",
    "items_data = items_data.drop(['movie_id', 'title'], axis=1)\n",
    "\n",
    "print(\"Ratings shape:\", ratings_data.shape, \"\\nUsers shape:\", users_data.shape, \"\\nItems shape:\", items_data.shape)\n",
    "\n",
    "# A quick look at the data\n",
    "display(ratings_data.head(3), users_data.head(3), items_data.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oF7qKPQlHAkx",
    "outputId": "6c057e7f-d5d9-425c-bc34-2ca785b73c47"
   },
   "outputs": [],
   "source": [
    "# ratings\n",
    "y_ratings_train, y_ratings_test = train_test_split(ratings_data, test_size=0.2, shuffle=True, random_state=42)\n",
    "y_ratings_val, y_ratings_test = train_test_split(y_ratings_test, test_size=0.5, shuffle=True, random_state=42)\n",
    "\n",
    "# users\n",
    "X_users_train, X_users_test = train_test_split(users_data, test_size=0.2, shuffle=True, random_state=42)\n",
    "X_users_val, X_users_test = train_test_split(X_users_test, test_size=0.5, shuffle=True, random_state=42)\n",
    "\n",
    "# items\n",
    "X_items_train, X_items_test = train_test_split(items_data, test_size=0.2, shuffle=True, random_state=42)\n",
    "X_items_val, X_items_test = train_test_split(X_items_test, test_size=0.5, shuffle=True, random_state=42)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X_users_train, X_users_val, X_users_test = X_users_train.values, X_users_val.values, X_users_test.values\n",
    "X_items_train, X_items_val, X_items_test = X_items_train.values, X_items_val.values, X_items_test.values\n",
    "y_ratings_train, y_ratings_val, y_ratings_test = y_ratings_train.values, y_ratings_val.values, y_ratings_test.values\n",
    "\n",
    "# Shape of the data\n",
    "print(\"Ratings shape:\", y_ratings_train.shape, y_ratings_val.shape, y_ratings_test.shape)\n",
    "print(\"Users shape:\", X_users_train.shape, X_users_val.shape, X_users_test.shape)\n",
    "print(\"Items shape:\", X_items_train.shape, X_items_val.shape, X_items_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kqdnbauU-hUd"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error, roc_auc_score\n",
    "\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "# from .utils import Utils, EarlyStopping\n",
    "from typing import Literal\n",
    "\n",
    "__model_version__ = '1.0.2'\n",
    "\n",
    "class NCF(nn.Module):\n",
    "    '''\n",
    "    Main Ranking model\n",
    "    '''\n",
    "    def __init__(self, mode: Literal['explicit', 'implicit'], num_users=6040, num_items=3952, user_dim=48, item_dim=19, num_factors=32, criterion=None, dropout=0.1, lr=1e-3, weight_decay=1e-5, verbose=False, gpu=True):\n",
    "        super(NCF, self).__init__()\n",
    "\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.mode = mode\n",
    "\n",
    "        # Embedding layers\n",
    "        self.user_embedding_mlp = nn.Embedding(num_users+1,  num_factors)\n",
    "        self.item_embedding_mlp = nn.Embedding(num_items+1,  num_factors)\n",
    "\n",
    "        self.user_embedding_mf = nn.Embedding(num_users+1, num_factors)\n",
    "        self.item_embedding_mf = nn.Embedding(num_items+1, num_factors)\n",
    "\n",
    "        # Fully connected layers for user/item features\n",
    "        self.user_features = nn.Sequential(\n",
    "            nn.Linear(user_dim, num_factors*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_factors*2, num_factors),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.item_features = nn.Sequential(\n",
    "            nn.Linear(item_dim, num_factors*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_factors*2, num_factors),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # MLP\n",
    "        self.MLP = nn.Sequential(\n",
    "            nn.Linear(4 * num_factors, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, num_factors),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # NeuMF layer\n",
    "        self.neu_mf = nn.Linear(2 * num_factors, 1)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr, weight_decay=weight_decay) # weight_decay is L2 regularization\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() and gpu else 'cpu')\n",
    "\n",
    "        self.__init_weights()\n",
    "\n",
    "        self.to(self.device)\n",
    "\n",
    "        # Print the model architecture\n",
    "        print(self, '\\nRunning on: ', self.device) if verbose else None\n",
    "        print(f'Number of parameters: {self.params_count():,}') if verbose else None\n",
    "\n",
    "    def __init_data(self, input, y=None):\n",
    "        '''\n",
    "        Initialize the data\n",
    "        '''\n",
    "        X_user_id, X_item_id, X_user, X_item = input\n",
    "\n",
    "        X_user = torch.FloatTensor(X_user).to(self.device, non_blocking=True)\n",
    "        X_item = torch.FloatTensor(X_item).to(self.device, non_blocking=True)\n",
    "        X_user_id = torch.IntTensor(X_user_id).to(self.device, non_blocking=True)\n",
    "        X_item_id = torch.IntTensor(X_item_id).to(self.device, non_blocking=True)\n",
    "        y = torch.FloatTensor(y).to(self.device, non_blocking=True) if y is not None else None\n",
    "\n",
    "\n",
    "        return X_user_id, X_item_id, X_user, X_item, y\n",
    "\n",
    "    def __init_weights(self) -> None:\n",
    "        '''\n",
    "        Initialize the weights\n",
    "        '''\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Embedding):\n",
    "                nn.init.normal_(m.weight, mean=0, std=0.01) # Normal initialization with mean 0 and standard deviation 0.01\n",
    "\n",
    "        for m in self.MLP.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_uniform_(m.weight, mode='fan_in', nonlinearity='relu') # He initialization, fan_in preserves the magnitude of the variance of the weights in the forward pass\n",
    "                m.bias.data.fill_(0.01)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.neu_mf.weight, gain=nn.init.calculate_gain('sigmoid')) # Glorot initialization\n",
    "\n",
    "    def forward(self, user_id: torch.IntTensor, item_id: torch.IntTensor, user_features: torch.FloatTensor, item_features: torch.FloatTensor, weights=None)-> torch.Tensor:\n",
    "        '''\n",
    "        Forward pass\n",
    "        '''\n",
    "        # Embedding Layers\n",
    "        if not weights:\n",
    "            user_embedding_mlp = self.user_embedding_mlp(user_id) # (batch_size, num_factors)\n",
    "            user_embedding_mf = self.user_embedding_mf(user_id) # (batch_size, num_factors)\n",
    "        else: # Explicit embeddings for OOV users\n",
    "            user_embedding_mlp = torch.tensor(weights[0], device=self.device).repeat(len(user_id), 1) # (batch_size, num_factors)\n",
    "            user_embedding_mf = torch.tensor(weights[1], device=self.device).repeat(len(user_id), 1) # (batch_size, num_factors)\n",
    "\n",
    "        item_embedding_mlp = self.item_embedding_mlp(item_id) # (batch_size, num_factors)\n",
    "        item_embedding_mf = self.item_embedding_mf(item_id) # (batch_size, num_factors)\n",
    "\n",
    "        # User and Item Features\n",
    "        user_features = self.user_features(user_features) # (batch_size, num_factors)\n",
    "        item_features = self.item_features(item_features) # (batch_size, num_factors)\n",
    "\n",
    "        # Concatenate the embeddings and features\n",
    "        user_embedding_mlp = torch.cat([user_embedding_mlp, user_features], dim=-1) # (batch_size, 2 * num_factors)\n",
    "        item_embedding_mlp = torch.cat([item_embedding_mlp, item_features], dim=-1) # (batch_size, 2 * num_factors)\n",
    "\n",
    "        # MLP Branch\n",
    "        mlp_input = torch.cat([user_embedding_mlp, item_embedding_mlp], dim=-1) # (batch_size, 4 * num_factors)\n",
    "        mlp_output = self.MLP(mlp_input) # (batch_size, num_factors)\n",
    "\n",
    "        # GMF Branch\n",
    "        mf_output = torch.mul(user_embedding_mf, item_embedding_mf) # (batch_size, num_factors)\n",
    "\n",
    "        # NeuMF Layer\n",
    "        neu_mf_input = torch.cat([mlp_output, mf_output], dim=-1) # (batch_size, 2 * num_factors)\n",
    "        neu_mf = self.neu_mf(neu_mf_input) # (batch_size, 1)\n",
    "\n",
    "        return self.sigmoid(neu_mf).flatten()\n",
    "    def get_user_embedding(self, user_id):\n",
    "        self.eval()  \n",
    "        with torch.no_grad():\n",
    "            user_id_tensor = torch.tensor([user_id], dtype=torch.long, device=self.device)\n",
    "            user_embedding = self.user_embedding_mf(user_id_tensor)\n",
    "            return user_embedding.squeeze(0).detach().cpu().numpy()\n",
    "            \n",
    "    def get_item_embedding(self, item_id):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            item_id_tensor = torch.tensor([item_id], dtype=torch.long, device=self.device)\n",
    "            item_embedding = self.item_embedding_mf(item_id_tensor)\n",
    "            return item_embedding.squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "    def fit(self, X: list[np.ndarray], y: np.ndarray, epochs: int, batch_size: int, X_val: list[np.ndarray] = None, y_val: np.ndarray = None, k:int = 10, scheduler: torch.optim.lr_scheduler = None, early_stopping: EarlyStopping = None) -> dict:\n",
    "        '''\n",
    "        Train the model\n",
    "\n",
    "        params:\n",
    "        X: list[np.ndarray] - [user_ids, item_ids, user_features, item_features]\n",
    "        y: np.ndarray - Target values\n",
    "        epochs: int - Number of epochs\n",
    "        batch_size: int - Batch size\n",
    "        k: int - Number of top-k items to consider\n",
    "        X_val: list[np.ndarray] - List of users and items features for validation\n",
    "        y_val: np.ndarray - Target values for validation\n",
    "\n",
    "        returns:\n",
    "        list[float] - Loss values for plotting\n",
    "        '''\n",
    "        X_user_id, X_item_id, X_user, X_item, y = self.__init_data(X, y)\n",
    "\n",
    "        dataset = TensorDataset(X_user_id, X_item_id, X_user, X_item, y)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        self.train()\n",
    "\n",
    "        # self.__init_weights() # Reinitialize the weights to prevent using the weights from the previous training session\n",
    "\n",
    "        losses = []\n",
    "        all_metrics = []\n",
    "        lrs = []\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0.0\n",
    "            for i, batch in enumerate(dataloader):\n",
    "                X_user_id, X_item_id, X_user, X_item, y = batch\n",
    "\n",
    "                self.optimizer.zero_grad() # Zero the gradients\n",
    "                output = self(X_user_id, X_item_id, X_user, X_item) # Forward pass\n",
    "                loss = self.criterion(output, y) # Compute the loss\n",
    "                loss.backward() # Backward pass\n",
    "                self.optimizer.step() # Update the weights\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                print(f'Epoch {epoch+1}/{epochs}') if i == 0 else None\n",
    "                print(f'{i+1}/{len(dataloader)} - loss: {(total_loss / (i+1)):.4f}', end='\\r' if i + 1 < len(dataloader) else ' ')\n",
    "\n",
    "            losses.append(total_loss / (i + 1))\n",
    "            # Evaluate the model\n",
    "            if X_val is not None and y_val is not None:\n",
    "                self.eval()\n",
    "                metrics = self.evaluate(X_val, y_val, batch_size, k)\n",
    "\n",
    "                if scheduler:\n",
    "                    scheduler.step(metrics[0])\n",
    "                    lrs.append(scheduler.get_last_lr()[0])\n",
    "\n",
    "                if self.mode == 'explicit':\n",
    "                    all_metrics.append([*metrics])\n",
    "                    print(f'- Val Loss: {metrics[0]:.4f} - R2: {metrics[1]:.4f} - MAE: {metrics[2]:.4f} - MSE: {metrics[3]:.4f} - RMSE: {metrics[4]:.4f} - lr: {lrs[-1]}')\n",
    "                else:\n",
    "                    all_metrics.append([*metrics])\n",
    "                    print(f'- Val Loss: {metrics[0]:.4f} - NDCG: {metrics[1]:.4f} - HR: {metrics[2]:.4f} - ROC-AUC: {metrics[3]:.4f} - lr: {lrs[-1]}')\n",
    "\n",
    "                if early_stopping:\n",
    "                    early_stopping(metrics[0], self)\n",
    "                    if early_stopping.early_stop:\n",
    "                        print(\"Early stopping\")\n",
    "                        break\n",
    "\n",
    "                self.train()\n",
    "            else:\n",
    "                print()\n",
    "\n",
    "        if self.mode == 'explicit':\n",
    "            history = {\n",
    "                'loss': losses,\n",
    "                'val_loss': [m[0] for m in all_metrics],\n",
    "                'r2': [m[1] for m in all_metrics],\n",
    "                'mae': [m[2] for m in all_metrics],\n",
    "                'mse': [m[3] for m in all_metrics],\n",
    "                'rmse': [m[4] for m in all_metrics],\n",
    "                'lr': lrs\n",
    "            }\n",
    "        else:\n",
    "            history = {\n",
    "                'loss': losses,\n",
    "                'val_loss': [m[0] for m in all_metrics],\n",
    "                'ndcg': [m[1] for m in all_metrics],\n",
    "                'hr': [m[2] for m in all_metrics],\n",
    "                'roc_auc': [m[3] for m in all_metrics],\n",
    "                'lr': lrs\n",
    "            }\n",
    "\n",
    "        return history\n",
    "\n",
    "    def predict(self, user_id: torch.IntTensor, item_id:torch.IntTensor, user: torch.FloatTensor, items: torch.FloatTensor)-> torch.Tensor:\n",
    "        '''\n",
    "        Alias for forward method, Initializes data first then calls forward method\n",
    "        '''\n",
    "        user_id, item_id, user_input, item_input, _ = self.__init_data([user_id, item_id, user, items])\n",
    "        return self(user_id, item_id, user_input, item_input)\n",
    "\n",
    "    def evaluate(self, X_val: list, y_val: np.ndarray, batch_size: int, k: int = None) -> tuple[float]:\n",
    "        '''\n",
    "        Evaluate the model\n",
    "\n",
    "        params:\n",
    "        X_val: list - List of users and items features\n",
    "        y_val: np.ndarray - Target values\n",
    "        batch_size: int - Batch size\n",
    "        type: Literal['explicit', 'implicit'] - Type of model (explicit or implicit)\n",
    "        k: int - Number of top-k items to consider\n",
    "        thresh: float - Threshold for hit rate\n",
    "\n",
    "        returns:\n",
    "        tuple[float] - Val loss, MAE, R2, RMSE, MSE for explicit model\n",
    "        tuple[float] - Val loss, NDCG, HR, AUC for implicit model\n",
    "        '''\n",
    "        user_id, item_id, user_input, item_input, target = self.__init_data(X_val, y_val)\n",
    "\n",
    "        dataset = TensorDataset(user_id, item_id, user_input, item_input, target)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "        avg_loss, y_preds = 0.0, []\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(dataloader):\n",
    "                user_id, item_id, user_input, item_input, y = batch\n",
    "\n",
    "                # Predictions\n",
    "                preds = self(user_id, item_id, user_input, item_input)\n",
    "\n",
    "                # Append to y_preds\n",
    "                y_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "                # Compute loss\n",
    "                loss = self.criterion(preds, y)\n",
    "                avg_loss += loss.item()\n",
    "\n",
    "        avg_loss /= (i + 1)\n",
    "\n",
    "        target = target.cpu().numpy()\n",
    "        y_preds = np.array(y_preds).reshape(-1)\n",
    "\n",
    "        # Compute additional evaluation metrics\n",
    "        if self.mode == 'explicit':\n",
    "            r2 = r2_score(target, y_preds)\n",
    "            mae = mean_absolute_error(target, y_preds)\n",
    "            mse = mean_squared_error(target, y_preds)\n",
    "            rmse = sqrt(mse)\n",
    "\n",
    "            return avg_loss, r2, mae, mse, rmse\n",
    "        else:\n",
    "            ndcg, hr = Utils.ndcg_hit_ratio(y_preds, X_val[2], y_val, k)\n",
    "            roc_auc = roc_auc_score(target, y_preds)\n",
    "\n",
    "            return avg_loss, ndcg, hr, roc_auc\n",
    "\n",
    "    def params_count(self)-> int:\n",
    "        '''\n",
    "        Count the number of parameters in the model\n",
    "        '''\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "    def save_weights(self, path)-> None:\n",
    "        '''\n",
    "        Save the model weights\n",
    "        '''\n",
    "        torch.save(self.state_dict(), path)\n",
    "\n",
    "    def load_weights(self, path, eval=True)-> None:\n",
    "        '''\n",
    "        Load the model weights\n",
    "\n",
    "        Note: Set eval to True if you want to set the model to evaluation mode (dropout layers will be disabled)\n",
    "        '''\n",
    "        self.load_state_dict(torch.load(path, map_location=self.device))\n",
    "        self.eval() if eval else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import random\n",
    "\n",
    "class HybridRecommender:\n",
    "    def __init__(self, neu_mf_model, rules, user_item_matrix, ratings_df, movies_df, X_users_train, X_items_train):\n",
    "        self.neu_mf = neu_mf_model\n",
    "        self.rules = rules\n",
    "        self.user_item_matrix = user_item_matrix\n",
    "        self.ratings_df = ratings_df\n",
    "        self.movies_df = movies_df\n",
    "        self.X_users_train = X_users_train   # <<< هنا أضفناهم\n",
    "        self.X_items_train = X_items_train   # <<< هنا أضفناهم\n",
    "#         self.all_item_ids = ratings_df['movie_id'].unique() \n",
    "        self.all_item_ids = list(ratings_df['movie_id'].unique())\n",
    "\n",
    "        self._prepare_knn()\n",
    "        self._validate_data()\n",
    "        self.item_counts = defaultdict(int)  \n",
    "        self.item_rewards = defaultdict(float)\n",
    "\n",
    "    def update_reward(self, item_id, reward):\n",
    "        \"\"\"تحديث المكافأة بعد تفاعل المستخدم\"\"\"\n",
    "        self.item_counts[item_id] += 1\n",
    "        self.item_rewards[item_id] += reward\n",
    "\n",
    "    def _validate_data(self):\n",
    "        \"\"\"تحقق من تكامل البيانات\"\"\"\n",
    "        required_movie_cols = {'movie_id', 'title', 'genres'}\n",
    "        if not required_movie_cols.issubset(self.movies_df.columns):\n",
    "            missing = required_movie_cols - set(self.movies_df.columns)\n",
    "            raise ValueError(f\"بيانات الأفلام تفتقد الأعمدة: {missing}\")\n",
    "\n",
    "        if not all(isinstance(rule, (list, tuple)) and len(rule)==3 for rule in self.rules):\n",
    "            raise ValueError(\"القواعد يجب أن تكون بصيغة (antecedents, consequents, confidence)\")\n",
    "    \n",
    "\n",
    "    def _prepare_knn(self):\n",
    "        \"\"\"تهيئة متكاملة لنموذج KNN مع PCA اختياري\"\"\"\n",
    "        self.item_ids = np.array(list(self.ratings_df['movie_id'].unique()))\n",
    "        self.item_embeddings = {}\n",
    "        valid_items = []\n",
    "        embeddings = []\n",
    "\n",
    "        # 1. جمع التضمينات الصالحة\n",
    "        for item in self.item_ids:\n",
    "            try:\n",
    "                emb = self.neu_mf.get_item_embedding(item)\n",
    "                self.item_embeddings[item] = emb\n",
    "                valid_items.append(item)\n",
    "                embeddings.append(emb)\n",
    "            except Exception as e:\n",
    "                print(f\"تحذير: خطأ في الحصول على تضمين للفيلم {item}: {str(e)}\")\n",
    "\n",
    "        if not valid_items:\n",
    "            raise ValueError(\"لا توجد عناصر صالحة لتهيئة KNN\")\n",
    "\n",
    "        self.item_ids = np.array(valid_items)\n",
    "        embeddings = np.array(embeddings)\n",
    "\n",
    "        # 2. تطبيق PCA اختياري عندما تكون الأبعاد كبيرة\n",
    "        self.pca = None  # هنا نحجز مكان لـ pca داخل الكائن نفسه\n",
    "        if embeddings.shape[1] > 50:  # إذا كان بعد التضمين > 50\n",
    "            from sklearn.decomposition import PCA\n",
    "            self.pca = PCA(n_components=min(32, embeddings.shape[1]))\n",
    "            embeddings = self.pca.fit_transform(embeddings)\n",
    "            print(f\"تم تقليل الأبعاد من {self.pca.n_components_} إلى {embeddings.shape[1]} باستخدام PCA\")\n",
    "\n",
    "        # 3. تهيئة نموذج KNN\n",
    "        self.knn_model = NearestNeighbors(\n",
    "            n_neighbors=50,\n",
    "            metric='cosine',\n",
    "            algorithm='brute'\n",
    "        )\n",
    "        self.knn_model.fit(embeddings)\n",
    "\n",
    "        # 4. حفظ التضمينات المعدلة\n",
    "        for i, item in enumerate(valid_items):\n",
    "            self.item_embeddings[item] = embeddings[i]\n",
    "        \n",
    "    def get_neu_mf_recommendations(self, user_id, k=10):\n",
    "        \"\"\"الحصول على توصيات من NeuMF\"\"\"\n",
    "        user_tensor = torch.tensor([user_id], dtype=torch.long)\n",
    "        user_features = torch.tensor([self.X_users_train[user_id]], dtype=torch.float32)\n",
    "\n",
    "        predictions = []\n",
    "        for item in self.all_item_ids:\n",
    "            item_tensor = torch.tensor([item], dtype=torch.long)\n",
    "            item_features = torch.tensor([self.X_items_train[item]], dtype=torch.float32)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                pred = self.neu_mf(user_tensor, item_tensor, user_features, item_features)\n",
    "            predictions.append((item, pred.item()))\n",
    "\n",
    "        predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [x[0] for x in predictions[:k]], [x[1] for x in predictions[:k]]\n",
    "    \n",
    "    def get_rule_based_recommendations(self, item_id):\n",
    "        \"\"\"الحصول على توصيات من القواعد الترابطية\"\"\"\n",
    "        recommendations = set()\n",
    "        for rule in self.rules:\n",
    "            antecedents, consequents, confidence = rule\n",
    "            if item_id in antecedents:\n",
    "                for item in consequents:\n",
    "                    if item != item_id:\n",
    "                        recommendations.add((item, confidence))\n",
    "        return list(recommendations)\n",
    "\n",
    "    def get_knn_recommendations(self, item_id, k=5):\n",
    "        \"\"\"إصدار محسن مع معالجة للأخطاء\"\"\"\n",
    "        if not hasattr(self, 'item_ids') or not hasattr(self, 'item_embeddings'):\n",
    "            print(\"تحذير: لم يتم تهيئة نموذج KNN بشكل صحيح\")\n",
    "            return []\n",
    "\n",
    "        if item_id not in self.item_embeddings:\n",
    "            print(f\"تحذير: لا يوجد تمثيل للفيلم {item_id} في نموذج KNN\")\n",
    "            return []\n",
    "\n",
    "        try:\n",
    "            item_embedding = self.item_embeddings[item_id].reshape(1, -1)\n",
    "            distances, indices = self.knn_model.kneighbors(item_embedding, n_neighbors=k+1)\n",
    "\n",
    "            # استبعاد العنصر نفسه من النتائج\n",
    "            recommendations = []\n",
    "            for j, i in enumerate(indices[0][1:], 1):  # تخطي الأول (العنصر نفسه)\n",
    "                rec_item = self.item_ids[i]\n",
    "                recommendations.append((rec_item, 1 - distances[0][j]))\n",
    "\n",
    "            return recommendations\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"خطأ في الحصول على توصيات KNN: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def hybrid_recommend(self, user_id, item_id=None, k=10, epsilon=0.1):\n",
    "        if not item_id:\n",
    "            recs, scores = self.get_neu_mf_recommendations(user_id, k*2)\n",
    "            item_popularity = self.ratings_df['movie_id'].value_counts().to_dict()\n",
    "\n",
    "            combined = defaultdict(float)\n",
    "            for item, score in zip(recs, scores):\n",
    "                popularity_score = item_popularity.get(item, 0) / max(item_popularity.values())\n",
    "                combined[item] = score * 0.7 + popularity_score * 0.3\n",
    "\n",
    "            sorted_items = sorted(combined.items(), key=lambda x: x[1], reverse=True)[:k*2]\n",
    "        else:\n",
    "            neu_mf_items, neu_mf_scores = self.get_neu_mf_recommendations(user_id, k)\n",
    "\n",
    "            rule_items = self.get_rule_based_recommendations(item_id) if item_id else []\n",
    "            knn_items = self.get_knn_recommendations(item_id, k//2) if item_id else []\n",
    "\n",
    "            combined = defaultdict(float)\n",
    "            item_popularity = self.ratings_df['movie_id'].value_counts().to_dict()\n",
    "\n",
    "            for item, score in zip(neu_mf_items, neu_mf_scores):\n",
    "                combined[item] += score * 0.7\n",
    "\n",
    "            for item, confidence in rule_items:\n",
    "                combined[item] += confidence * 0.15\n",
    "\n",
    "            for item, similarity in knn_items:\n",
    "                combined[item] += similarity * 0.15\n",
    "\n",
    "            for item in combined:\n",
    "                combined[item] += 0.1 * (item_popularity.get(item, 0) / max(item_popularity.values()))\n",
    "\n",
    "            sorted_items = sorted(combined.items(), key=lambda x: x[1], reverse=True)[:k*2]\n",
    "\n",
    "        # إعادة ترتيب باستخدام Bandit\n",
    "        bandit_items = self.bandit_rerank(sorted_items, epsilon=epsilon, top_k=k)\n",
    "\n",
    "        items_already_seen_by_user = set(self.ratings_df[self.ratings_df['user_id'] == user_id]['movie_id'].values)\n",
    "\n",
    "        recommendations = []\n",
    "        for movie_id, score in bandit_items:\n",
    "            if movie_id in items_already_seen_by_user:\n",
    "                continue  # تجاهل الأفلام التي شاهدها المستخدم سابقاً\n",
    "\n",
    "            explanation = []\n",
    "            try:\n",
    "                movie = self.movies_df[self.movies_df['movie_id'] == movie_id].iloc[0]\n",
    "\n",
    "                if not item_id or movie_id in neu_mf_items[:5]:\n",
    "                    explanation.append(f\"Recommended because your preferences align with similar users (score: {score:.2f})\")\n",
    "\n",
    "                if item_id and movie_id in [x[0] for x in rule_items]:\n",
    "                    related_movies = [r[0] for r in rule_items if r[0] != movie_id][:3]\n",
    "                    related_titles = [self.movies_df[self.movies_df['movie_id'] == m].iloc[0]['title'] \n",
    "                                      for m in related_movies if m in self.movies_df['movie_id'].values]\n",
    "                    if related_titles:\n",
    "                        explanation.append(f\"We recommend this because you liked movies that are often watched together with: {', '.join(related_titles)}\")\n",
    "\n",
    "                if item_id and movie_id in [x[0] for x in knn_items]:\n",
    "                    similar_movies = [k[0] for k in knn_items if k[0] != movie_id][:3]\n",
    "                    similar_titles = [self.movies_df[self.movies_df['movie_id'] == m].iloc[0]['title'] \n",
    "                                      for m in similar_movies if m in self.movies_df['movie_id'].values]\n",
    "\n",
    "                    if similar_titles:\n",
    "                        explanation.append(f\"This is similar to what you liked before: {', '.join(similar_titles)}\")\n",
    "\n",
    "                genres = movie['genres'].split('|')[:3]\n",
    "\n",
    "                recommendations.append({\n",
    "                    'movie_id': movie_id,\n",
    "                    'title': movie['title'],\n",
    "                    'genres': ', '.join(genres),\n",
    "                    'score': round(score, 4),\n",
    "                    'explanation': \" - \".join(explanation) if explanation else \"Recommended due to general popularity and similarity to your past preferences.\"\n",
    "                })\n",
    "\n",
    "            except IndexError:\n",
    "                print(f\"WARNING: Movie {movie_id} skipped due to incomplete data\")\n",
    "                continue\n",
    "        return recommendations\n",
    "\n",
    "    def bandit_rerank(self, candidates, epsilon=0.1, top_k=10):\n",
    "        \"\"\"\n",
    "        إعادة ترتيب التوصيات باستخدام خوارزمية ε-Greedy Bandit\n",
    "        \"\"\"\n",
    "        selected = []\n",
    "        for _ in range(top_k):\n",
    "            if not candidates:\n",
    "                break  # \n",
    "                print(\"Warning: The candidate list became empty during sorting!\")\n",
    "            if random.random() < epsilon:\n",
    "                random_item = random.choice(candidates)\n",
    "                selected.append(random_item)\n",
    "            else:\n",
    "                scored_items = []\n",
    "                for item_id, score in candidates:\n",
    "                    count = self.item_counts[item_id]\n",
    "                    reward = self.item_rewards[item_id]\n",
    "                    avg_reward = reward / count if count > 0 else score\n",
    "                    scored_items.append((item_id, avg_reward))\n",
    "\n",
    "                best_item = sorted(scored_items, key=lambda x: x[1], reverse=True)[0]\n",
    "                selected.append(best_item)\n",
    "\n",
    "            # نحذف العنصر الذي اخترناه\n",
    "            candidates = [c for c in candidates if c[0] != selected[-1][0]]\n",
    "        return selected\n",
    "\n",
    "            \n",
    "    def update_bandit_feedback(self, item_id, reward):\n",
    "        self.item_counts[item_id] += 1\n",
    "        self.item_rewards[item_id] += reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dwYHgCPXoEFn",
    "outputId": "43f86014-ad7c-425d-bf6a-3b6e02c46516"
   },
   "outputs": [],
   "source": [
    "\n",
    "user_dim = users_data.shape[1] # 48\n",
    "item_dim = items_data.shape[1] # 19\n",
    "num_users = int(users_data_og['user_id'].max()) + 1  # +1 لأن الفهرس يبدأ من 0\n",
    "num_items = int(items_data_og['movie_id'].max()) + 1\n",
    "\n",
    "# num_users = users_data_og['user_id'].max()\n",
    "# num_items = items_data_og['movie_id'].max()\n",
    "\n",
    "embedding_dim = 32 # number of latent factors\n",
    "\n",
    "model = NCF(\n",
    "    num_users=num_users,\n",
    "    num_items=num_items,\n",
    "    user_dim=user_dim,\n",
    "    item_dim=item_dim,\n",
    "    num_factors=embedding_dim,\n",
    "    mode='implicit',\n",
    "    criterion=torch.nn.BCELoss(), # BCELoss since output layer has sigmoid applied to it, otherwise BCEWithLogitsLoss()\n",
    "    dropout=0.1,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-5, # L2 regularization\n",
    "    verbose=True,\n",
    "    gpu=True\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(patience=2, delta=0.001, path='C:/Users/lenovo/Desktop/LinUCB-HybridRecommender/implicit.pth') # early stops after 2 consecutive epochs with minor loss decrease/increase\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(model.optimizer, mode='min', factor=0.1, patience=0) # reduces learning rate by factor of 0.1 when no improvement is seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "def generate_association_rules(user_item_matrix, min_support=0.01, min_confidence=0.2, max_len=2):\n",
    "    \"\"\"\n",
    "    توليد قواعد ارتباطية من مصفوفة تفاعلات المستخدمين والعناصر\n",
    "    \n",
    "    Args:\n",
    "        user_item_matrix: مصفوفة تفاعلات (مستخدمين × عناصر) - يجب أن تكون pandas.DataFrame\n",
    "        min_support: الحد الأدنى للدعم\n",
    "        min_confidence: الحد الأدنى للثقة\n",
    "        max_len: الطول الأقصى للقاعدة\n",
    "    \n",
    "    Returns:\n",
    "        list: قواعد الارتباط [(antecedents, consequents, confidence), ...]\n",
    "    \"\"\"\n",
    "    # تحويل المصفوفة إلى DataFrame إذا لم تكن كذلك\n",
    "    if not isinstance(user_item_matrix, pd.DataFrame):\n",
    "        user_item_matrix = pd.DataFrame.sparse.from_spmatrix(user_item_matrix)\n",
    "    \n",
    "    # تحويل القيم إلى بوليان (True/False)\n",
    "    binary_matrix = user_item_matrix.astype(bool)\n",
    "    \n",
    "    # إيجاد العناصر المتكررة\n",
    "    frequent_itemsets = apriori(binary_matrix, \n",
    "                              min_support=min_support, \n",
    "                              use_colnames=True,\n",
    "                              max_len=max_len)\n",
    "    \n",
    "    # توليد القواعد الترابطية\n",
    "    rules = association_rules(frequent_itemsets, \n",
    "                            metric=\"confidence\", \n",
    "                            min_threshold=min_confidence)\n",
    "    \n",
    "    # تحويل النتائج إلى الشكل المطلوب\n",
    "    formatted_rules = []\n",
    "    for _, rule in rules.iterrows():\n",
    "        antecedents = list(rule['antecedents'])\n",
    "        consequents = list(rule['consequents'])\n",
    "        confidence = rule['confidence']\n",
    "        formatted_rules.append((antecedents, consequents, confidence))\n",
    "    \n",
    "    return formatted_rules\n",
    "\n",
    "# تعديل دالة إنشاء المصفوفة لتعيد DataFrame بدلاً من csr_matrix\n",
    "def create_user_item_matrix(ratings_df, num_users, num_items):\n",
    "    \"\"\"\n",
    "    إنشاء مصفوفة تفاعلات مستخدمين-عناصر كـ DataFrame\n",
    "    \"\"\"\n",
    "    sparse_matrix = csr_matrix((ratings_df['rating'], \n",
    "                             (ratings_df['user_id'], ratings_df['movie_id'])),\n",
    "                            shape=(num_users, num_items))\n",
    "    \n",
    "    return pd.DataFrame.sparse.from_spmatrix(sparse_matrix)\n",
    "\n",
    "user_item_matrix = create_user_item_matrix(ratings_data, num_users, num_items)\n",
    "rules = generate_association_rules(user_item_matrix, min_support=0.05, min_confidence=0.4, max_len=2)\n",
    "print(f\"Generated {len(rules)} association rules\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# 1. تدريب نموذج NeuMF\n",
    "history = model.fit(\n",
    "    X=[y_ratings_train[:, 0], y_ratings_train[:, 1], X_users_train, X_items_train],\n",
    "    y=y_ratings_train[:, 2],\n",
    "    X_val=[y_ratings_val[:, 0], y_ratings_val[:, 1], X_users_val, X_items_val],\n",
    "    y_val=y_ratings_val[:, 2],\n",
    "    epochs=1,\n",
    "    batch_size=2048,\n",
    "    k=10,  # إضافة هذه السطر\n",
    "    early_stopping=early_stopping,\n",
    "    scheduler=scheduler\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval() # set model to evaluation mode (disables dropout layers)\n",
    "\n",
    "# topk=10\n",
    "\n",
    "# avg_loss, ndcg, hr, auc = model.evaluate([\n",
    "#     y_ratings_test[:, 0],\n",
    "#     y_ratings_test[:, 1],\n",
    "#     X_users_test,\n",
    "#     X_items_test],\n",
    "#     y_ratings_test[:, 2],\n",
    "#     batch_size=256,\n",
    "#     k=topk\n",
    "# )\n",
    "\n",
    "# print(f\"Average Loss: {avg_loss:.4f}, NDCG@{topk}: {ndcg:.4f}, HR@{topk}: {hr:.4f}, AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = items_data_og[['movie_id', 'title', 'genre']].copy()\n",
    "movies_df.rename(columns={'movie_id': 'movie_id', 'genre': 'genres'}, inplace=True)\n",
    "\n",
    "# 2. إنشاء النظام الهجين\n",
    "hybrid_rec = HybridRecommender(\n",
    "    neu_mf_model=model,\n",
    "    rules=rules,\n",
    "    user_item_matrix=user_item_matrix,\n",
    "    ratings_df=ratings_data,\n",
    "    movies_df=movies_df,\n",
    "    X_users_train=X_users_train, \n",
    "    X_items_train=X_items_train     \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "recommendations = hybrid_rec.hybrid_recommend(user_id=50, item_id=40, k=10)\n",
    "\n",
    "# عرض النتائج\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"{i}. {rec['title']} (Score: {rec['score']:.3f})\")\n",
    "    print(f\"   Genres: {rec['genres']}\")\n",
    "    print(f\"   Explanation: {rec['explanation']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"=== analysis evaluation dis ===\")\n",
    "# print(\" train data :\")\n",
    "# print(pd.DataFrame(y_ratings_train[:,2]).describe())\n",
    "# print(\"\\n test data  :\")\n",
    "# print(pd.DataFrame(y_ratings_test[:,2]).describe())\n",
    "\n",
    "# # تحليل التوزيع\n",
    "# plt.figure(figsize=(12,5))\n",
    "# plt.subplot(1,2,1)\n",
    "# plt.hist(y_ratings_train[:,2], bins=10)\n",
    "# plt.title(\"dis evl train\")\n",
    "# plt.subplot(1,2,2)\n",
    "# plt.hist(y_ratings_test[:,2], bins=10)\n",
    "# plt.title(\"  dist \")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = hybrid_rec.hybrid_recommend(user_id=600, item_id=500, k=10)\n",
    "\n",
    "# عرض النتائج\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"{i}. {rec['title']} (Score: {rec['score']:.3f})\")\n",
    "    print(f\"   Genres: {rec['genres']}\")\n",
    "    print(f\"   Explanation: {rec['explanation']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = hybrid_rec.hybrid_recommend(user_id=3000, item_id=2000, k=10)\n",
    "\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    # تحسين عرض الأنواع\n",
    "    genres = rec['genres'].split('|')[:3]  # عرض أول 3 أنواع فقط\n",
    "    print(f\"{i}. {rec['title']} (Score: {rec['score']:.3f})\")\n",
    "    print(f\"   الأنواع: {', '.join(genres)}\")\n",
    "    print(f\"   التفسير: {rec['explanation']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import ndcg_score\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "class OptimizedEvaluator:\n",
    "    @staticmethod\n",
    "    def evaluate_model(recommender, test_data, train_data, k=10, model_type='hybrid', sample_size=None):\n",
    "        \"\"\"\n",
    "        Improved evaluation with the ability to take a random sample and store temporary results.\n",
    "        \"\"\"\n",
    "        # تحويل البيانات وتجهيزها\n",
    "        test_df = pd.DataFrame(test_data[:, :2], columns=['user_id', 'movie_id'])\n",
    "        train_df = pd.DataFrame(train_data[:, :2], columns=['user_id', 'movie_id'])\n",
    "        \n",
    "        # أخذ عينة عشوائية إذا طُلب\n",
    "        if sample_size and sample_size < len(test_df['user_id'].unique()):\n",
    "            users_sample = np.random.choice(test_df['user_id'].unique(), size=sample_size, replace=False)\n",
    "            test_df = test_df[test_df['user_id'].isin(users_sample)]\n",
    "        \n",
    "        # تجميع التفاعلات\n",
    "        train_user_items = train_df.groupby('user_id')['movie_id'].apply(set).to_dict()\n",
    "        test_user_items = test_df.groupby('user_id')['movie_id'].apply(set).to_dict()\n",
    "        \n",
    "        # إعداد قوائم النتائج\n",
    "        metrics = {\n",
    "            'precision': [],\n",
    "            'recall': [],\n",
    "            'hit': 0,\n",
    "            'ndcg': [],\n",
    "            'users_processed': 0\n",
    "        }\n",
    "        \n",
    "        # تقييم كل مستخدم مع شريط التقدم\n",
    "        for user_id, test_items in tqdm(test_user_items.items(), desc=f'Evaluating {model_type} (k={k})'):\n",
    "            try:\n",
    "                # الحصول على التوصيات\n",
    "                recs = OptimizedEvaluator._get_recommendations(\n",
    "                    recommender, user_id, test_items, k, model_type\n",
    "                )\n",
    "                print(f\"User {user_id} - Recommendations before filtering: {len(recs)}\")\n",
    "\n",
    "                # تصفية العناصر المسبوقة\n",
    "                seen_items = train_user_items.get(user_id, set())\n",
    "                recs = [item for item in recs if item not in seen_items][:k]\n",
    "                print(f\"User {user_id} - Recommendations after filtering: {len(recs)}\")\n",
    "\n",
    "                # حساب المقاييس\n",
    "                relevant = len(test_items & set(recs))\n",
    "                \n",
    "                metrics['precision'].append(relevant / len(recs) if recs else 0)\n",
    "                metrics['recall'].append(relevant / len(test_items) if test_items else 0)\n",
    "                \n",
    "                if relevant > 0:\n",
    "                    metrics['hit'] += 1\n",
    "                \n",
    "                # حساب NDCG المعدل\n",
    "                y_true = [1 if item in test_items else 0 for item in recs]\n",
    "                if y_true:\n",
    "                    if len(y_true) > 1:\n",
    "                        metrics['ndcg'].append(ndcg_score([y_true], [range(len(y_true), 0, -1)], k=k))\n",
    "                    else:\n",
    "                        metrics['ndcg'].append(1.0 if y_true[0] == 1 else 0.0)\n",
    "                \n",
    "                metrics['users_processed'] += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        # حساب المتوسطات\n",
    "        return {\n",
    "            'model_type': model_type,\n",
    "            'k': k,\n",
    "            'precision@k': np.mean(metrics['precision']) if metrics['precision'] else 0,\n",
    "            'recall@k': np.mean(metrics['recall']) if metrics['recall'] else 0,\n",
    "            'hit_ratio': metrics['hit'] / metrics['users_processed'] if metrics['users_processed'] > 0 else 0,\n",
    "            'ndcg@k': np.mean(metrics['ndcg']) if metrics['ndcg'] else 0,\n",
    "            'users_evaluated': metrics['users_processed']\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_recommendations(recommender, user_id, test_items, k, model_type):\n",
    "        \"\"\"مساعدة للحصول على التوصيات بناءً على نوع النموذج\"\"\"\n",
    "\n",
    "        if model_type == 'hybrid':\n",
    "            # استخدم أول عنصر من test_items إذا كان متاحًا، أو استخدم تفاعل سابق للمستخدم\n",
    "            first_item = next(iter(test_items)) if test_items else None\n",
    "            return [r['movie_id'] for r in recommender.hybrid_recommend(\n",
    "                user_id=int(user_id), \n",
    "                item_id=first_item, \n",
    "                k=k\n",
    "            )]\n",
    "\n",
    "        elif model_type == 'neumf':\n",
    "            # استرجاع التوصيات من نموذج Neural Matrix Factorization\n",
    "            rec_items, _ = recommender.get_neu_mf_recommendations(int(user_id), k)\n",
    "            return rec_items\n",
    "\n",
    "        elif model_type == 'rule':\n",
    "            if test_items:\n",
    "                # إذا كان هناك تفاعل في test_items، استخدم أول عنصر\n",
    "                first_item = next(iter(test_items)) if test_items else None\n",
    "                if first_item is None:\n",
    "                    return []\n",
    "\n",
    "                rule_recs = recommender.get_rule_based_recommendations(first_item)\n",
    "\n",
    "                # طباعة عدد التوصيات قبل التصفية\n",
    "                print(f\"User {user_id} - Rule Recommendations before filtering: {len(rule_recs)}\")\n",
    "\n",
    "                # تصفية التوصيات بناءً على العناصر المسبق تفاعل المستخدم معها\n",
    "                seen_items = recommender.get_user_seen_items(user_id)\n",
    "                rule_recs = [r[0] for r in rule_recs if r[0] not in seen_items][:k]\n",
    "\n",
    "                # طباعة عدد التوصيات بعد التصفية\n",
    "                print(f\"User {user_id} - Rule Recommendations after filtering: {len(rule_recs)}\")\n",
    "\n",
    "                return rule_recs\n",
    "            return []\n",
    "\n",
    "        elif model_type == 'knn':\n",
    "            if test_items:\n",
    "                # إذا كان هناك تفاعل في test_items، استخدم أول عنصر\n",
    "                first_item = next(iter(test_items)) if test_items else None\n",
    "                if first_item is None:\n",
    "                    return []\n",
    "\n",
    "                # استرجاع التوصيات باستخدام خوارزمية KNN\n",
    "                knn_recs = recommender.get_knn_recommendations(first_item, k)\n",
    "\n",
    "                # طباعة عدد التوصيات قبل التصفية\n",
    "                print(f\"User {user_id} - KNN Recommendations before filtering: {len(knn_recs)}\")\n",
    "\n",
    "                # تصفية التوصيات بناءً على العناصر التي تم التفاعل معها مسبقًا من قبل المستخدم\n",
    "                seen_items = recommender.get_user_seen_items(user_id)\n",
    "                knn_recs = [item for item in knn_recs if item not in seen_items][:k]\n",
    "\n",
    "                # طباعة عدد التوصيات بعد التصفية\n",
    "                print(f\"User {user_id} - KNN Recommendations after filtering: {len(knn_recs)}\")\n",
    "\n",
    "                return knn_recs\n",
    "            return []\n",
    "def run_evaluation(hybrid_rec, y_ratings_train, y_ratings_test, k_values=[5, 10], sample_size=2000):\n",
    "    \"\"\"\n",
    "    تشغيل التقييم مع إمكانية أخذ عينة وتخزين النتائج\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for k in k_values:\n",
    "        print(f\"\\nEvaluation for k={k} (sample size: {sample_size})\")\n",
    "        \n",
    "        for model_type in ['hybrid', 'neumf']:\n",
    "            result = OptimizedEvaluator.evaluate_model(\n",
    "                recommender=hybrid_rec,\n",
    "                test_data=y_ratings_test,\n",
    "                train_data=y_ratings_train,\n",
    "                k=k,\n",
    "                model_type=model_type,\n",
    "                sample_size=sample_size\n",
    "            )\n",
    "            results.append(result)\n",
    "            print(f\"  - {model_type}: \"\n",
    "                  f\"Precision@{k}={result['precision@k']:.3f}, \"\n",
    "                  f\"Recall@{k}={result['recall@k']:.3f}, \"\n",
    "                  f\"HitRatio={result['hit_ratio']:.3f}, \"\n",
    "                  f\"NDCG@{k}={result['ndcg@k']:.3f}\")\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# تشغيل التقييم مع معالجة الأخطاء\n",
    "try:\n",
    "    # استخدم عينة أصغر للتحليل السريع (يمكن زيادة sample_size لتحسين الدقة)\n",
    "    evaluation_results = run_evaluation(\n",
    "        hybrid_rec=hybrid_rec,\n",
    "        y_ratings_train=y_ratings_train,\n",
    "        y_ratings_test=y_ratings_test,\n",
    "        k_values=[10, 20, 50],\n",
    "        sample_size=6000  # يمكن زيادتها إلى 2000 أو أكثر لتحسين الدقة\n",
    "    )\n",
    "    \n",
    "    # تحليل النتائج\n",
    "    print(\"\\nFinal Results Summary:\")\n",
    "    print(evaluation_results[['model_type', 'k', 'precision@k', 'recall@k', 'hit_ratio', 'ndcg@k']])\n",
    "    \n",
    "    # تصور النتائج\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    metrics = ['precision@k', 'recall@k', 'hit_ratio', 'ndcg@k']\n",
    "    \n",
    "    for i, metric in enumerate(metrics, 1):\n",
    "        plt.subplot(2, 2, i)\n",
    "        sns.barplot(data=evaluation_results, x='model_type', y=metric, hue='k')\n",
    "        plt.title(f'{metric.upper()} Comparison')\n",
    "        plt.ylabel(metric)\n",
    "        plt.ylim(0, 1)\n",
    "        plt.legend(title='k value')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('model_comparison1m_20001.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # تحليل أداء النموذج الهجين\n",
    "    hybrid_results = evaluation_results[evaluation_results['model_type'] == 'hybrid']\n",
    "    print(\"\\nHybrid Model Performance:\")\n",
    "    print(hybrid_results)\n",
    "    \n",
    "    # حفظ النتائج\n",
    "    evaluation_results.to_csv('evaluation_results1m_20001.csv', index=False)\n",
    "    print(\"\\nResults saved to evaluation_resultsm_20001.csv\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nEvaluation failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
