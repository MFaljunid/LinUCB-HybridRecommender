{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 413
    },
    "id": "CkuKoMdNxJI1",
    "outputId": "2acb6aab-55d7-48e8-fcfd-696b78b1c13a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  movie_id  rating  timestamp\n",
       "0        1      1193       5  978300760\n",
       "1        1       661       3  978302109\n",
       "2        1       914       3  978301968"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>occupation</th>\n",
       "      <th>zip_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>56</td>\n",
       "      <td>16</td>\n",
       "      <td>70072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "      <td>55117</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id gender  age  occupation zip_code\n",
       "0        1      F    1          10    48067\n",
       "1        2      M   56          16    70072\n",
       "2        3      M   25          15    55117"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>title</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Animation|Children's|Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children's|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movie_id                    title                         genre\n",
       "0         1         Toy Story (1995)   Animation|Children's|Comedy\n",
       "1         2           Jumanji (1995)  Adventure|Children's|Fantasy\n",
       "2         3  Grumpier Old Men (1995)                Comedy|Romance"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000209, 4) (6040, 5) (3883, 3)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import display\n",
    "\n",
    "import torch\n",
    "\n",
    "# تحميل البيانات\n",
    "cols_dict = {\n",
    "    'ratings': ['user_id', 'movie_id', 'rating', 'timestamp'],\n",
    "    'users': ['user_id', 'gender', 'age', 'occupation', 'zip_code'],\n",
    "    'items': ['movie_id', 'title', 'genre']\n",
    "}\n",
    "dir = 'C:/Users/lenovo/Desktop/LinUCB-HybridRecommender/data/1m/'\n",
    "\n",
    "ratings_data = pd.read_csv(dir + 'ratings.dat', sep='::', names=cols_dict['ratings'], engine='python')\n",
    "users_data = pd.read_csv(dir + 'users.dat', sep='::', names=cols_dict['users'], engine='python')\n",
    "items_data = pd.read_csv(dir + 'movies.dat', sep='::', names=cols_dict['items'], encoding='latin-1', engine='python')\n",
    "\n",
    "display(ratings_data.head(3), users_data.head(3), items_data.head(3))\n",
    "print(ratings_data.shape, users_data.shape, items_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import torch\n",
    "\n",
    "# # تعريف هيكل البيانات\n",
    "# cols_dict = {\n",
    "#     'ratings': ['user_id', 'movie_id', 'rating', 'timestamp'],\n",
    "#     'users': ['user_id', 'gender', 'age', 'occupation', 'zip_code'],\n",
    "#     'items': ['movie_id', 'title', 'genre']\n",
    "# }\n",
    "\n",
    "# # المسار إلى ملفات البيانات 100K\n",
    "# dir = 'C:/Users/lenovo/Desktop/LinUCB-HybridRecommender/data/100k/'  # تغيير المسار هنا\n",
    "\n",
    "# # تحميل البيانات مع معالجة الأخطاء المحتملة\n",
    "# try:\n",
    "#     ratings_data = pd.read_csv(dir + 'u.data', sep='\\t', names=cols_dict['ratings'])\n",
    "#     users_data = pd.read_csv(dir + 'u.user', sep='|', names=cols_dict['users'])\n",
    "#     items_data = pd.read_csv(dir + 'u.item', sep='|', names=cols_dict['items'] + ['other_details'], encoding='latin-1')\n",
    "    \n",
    "#     # تنظيف بيانات الأفلام (قد تختلف حسب هيكل ملف u.item)\n",
    "#     items_data = items_data[cols_dict['items']]  # الاحتفاظ بالأعمدة المطلوبة فقط\n",
    "    \n",
    "#     print(\"تم تحميل البيانات بنجاح!\")\n",
    "#     print(f\"أبعاد البيانات: {ratings_data.shape} (تقييمات), {users_data.shape} (مستخدمين), {items_data.shape} (أفلام)\")\n",
    "    \n",
    "# except Exception as e:\n",
    "#     print(f\"حدث خطأ أثناء تحميل البيانات: {str(e)}\")\n",
    "#     # تحميل بيانات مثاليه في حالة الخطأ (اختياري)\n",
    "#     # ratings_data, users_data, items_data = load_sample_data()\n",
    "\n",
    "# # عرض عينة من البيانات\n",
    "# display(ratings_data.head(3))\n",
    "# display(users_data.head(3))\n",
    "# display(items_data.head(3))\n",
    "\n",
    "# # معلومات إضافية عن البيانات\n",
    "# print(\"\\nمعلومات إضافية:\")\n",
    "# print(f\"عدد المستخدمين الفريدين: {ratings_data['user_id'].nunique()}\")\n",
    "# print(f\"عدد الأفلام الفريدة: {ratings_data['movie_id'].nunique()}\")\n",
    "# print(f\"النطاق الزمني للتقييمات: من {pd.to_datetime(ratings_data['timestamp'], unit='s').min()} إلى {pd.to_datetime(ratings_data['timestamp'], unit='s').max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7LivXHwsx8Dp",
    "outputId": "b5f1ba25-4952-4084-86e8-0e892cda2158"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['user_id', 'gender', 'age', 'occupation', 'zip_code'], dtype='object')\n",
      "Index(['movie_id', 'title', 'genre'], dtype='object')\n",
      "Index(['user_id', 'movie_id', 'rating', 'timestamp'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# نسخة احتياطية\n",
    "users_data_og = users_data.copy(deep=True)\n",
    "items_data_og = items_data.copy(deep=True)\n",
    "ratings_data_og = ratings_data.copy(deep=True)\n",
    "print(users_data.columns)\n",
    "print(items_data.columns)\n",
    "print(ratings_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "10zoKlg6ygc2"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Literal, Union\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import ndcg_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class Utils:\n",
    "    @staticmethod\n",
    "    def extract_year(items_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        '''\n",
    "        Extracts:\n",
    "            - Year from title\n",
    "\n",
    "        returns: Dataframes with extracted features\n",
    "        '''\n",
    "        # Extract year from title\n",
    "        items_df['year'] = items_df['title'].str.extract(r'\\((\\d{4})\\)').astype(int)\n",
    "\n",
    "        return items_df\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_category_avg_ratings(users_df: pd.DataFrame, items_df: pd.DataFrame, ratings_df: pd.DataFrame, k=0.6) -> pd.DataFrame:\n",
    "        '''\n",
    "        Extracts penalized average ratings for each category for each user using exponential decay penalty.\n",
    "\n",
    "        users_df: DataFrame containing user information.\n",
    "        items_df: DataFrame containing item information with categories as binary columns.\n",
    "        ratings_df: DataFrame containing user-item interactions and ratings.\n",
    "        k: Control factor for penalty steepness. Default is 0.6.\n",
    "\n",
    "        Returns a DataFrame with users and their penalized average ratings per category.\n",
    "        '''\n",
    "        # Create a copy of users_df to store features\n",
    "        features_df = users_df.copy()\n",
    "\n",
    "        # Define exponential penalty function\n",
    "        def exp_penalty(n, k=0.6):\n",
    "            return 1 / np.exp(k * n)\n",
    "\n",
    "        # Iterate over each category in the items_df (excluding non-categorical columns)\n",
    "        for category in items_df.columns[2:]:\n",
    "            # Get item IDs in the current category\n",
    "            category_items = items_df[items_df[category] == 1]['movie_id']\n",
    "\n",
    "            # Filter ratings_df to include only ratings for items in the current category\n",
    "            category_ratings = ratings_df[ratings_df['movie_id'].isin(category_items)]\n",
    "\n",
    "            # Group by user_id and calculate the average rating and count of ratings for the current category\n",
    "            user_stats = category_ratings.groupby('user_id')['rating'].agg(['mean', 'count']).reset_index()\n",
    "            user_stats.columns = ['user_id', f'user_avg_rating_{category}', f'count_rating_{category}']\n",
    "\n",
    "            # Merge the user stats into features_df\n",
    "            features_df = pd.merge(features_df, user_stats, on='user_id', how='left')\n",
    "\n",
    "            # Apply exponential penalty and calculate the penalized average rating for each user\n",
    "            features_df[f'user_avg_rating_{category}'] = (\n",
    "                (1 - exp_penalty(features_df[f'count_rating_{category}'], k)) * features_df[f'user_avg_rating_{category}']\n",
    "            )\n",
    "\n",
    "            # Fill missing values with 0\n",
    "            features_df[f'user_avg_rating_{category}'] = features_df[f'user_avg_rating_{category}'].fillna(0)\n",
    "\n",
    "        # Select only the relevant columns\n",
    "        cols = features_df.columns[:32].tolist() + [col for col in features_df.columns if col.startswith('user_avg_rating_')]\n",
    "        result_df = features_df[cols]\n",
    "\n",
    "        return result_df\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_category_freq(users_df: pd.DataFrame, items_df: pd.DataFrame, ratings_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        # Copy users_df to avoid modifying the original dataframe\n",
    "        freq_df = users_df.copy()\n",
    "\n",
    "        # Get total interactions for each user\n",
    "        total_interactions = ratings_df.groupby('user_id').size().reset_index(name='total_interactions')\n",
    "        freq_df = pd.merge(freq_df, total_interactions, on='user_id', how='left').fillna(0)\n",
    "\n",
    "        # Iterate over each category in the items_df (assuming categories are from the 3rd column onward)\n",
    "        for category in items_df.columns[2:]:\n",
    "            # Get movie_ids that belong to the current category\n",
    "            movie_ids_in_category = items_df[items_df[category] == 1]['movie_id']\n",
    "\n",
    "            # Count interactions in the current category for each user\n",
    "            category_interactions = ratings_df[ratings_df['movie_id'].isin(movie_ids_in_category)].groupby('user_id').size().reset_index(name=f'{category}_count')\n",
    "\n",
    "            # Merge category_interactions with freq_df\n",
    "            freq_df = pd.merge(freq_df, category_interactions, on='user_id', how='left').fillna(0)\n",
    "\n",
    "            # Calculate frequency of interactions for the current category\n",
    "            freq_df[f'freq_{category}'] = freq_df[f'{category}_count'] / freq_df['total_interactions']\n",
    "\n",
    "            # Drop the intermediate category count column\n",
    "            freq_df.drop(columns=[f'{category}_count'], inplace=True)\n",
    "\n",
    "        return freq_df.fillna(0).drop(columns=['total_interactions'])\n",
    "\n",
    "    @staticmethod\n",
    "    def extend_users_items(users_df: pd.DataFrame, items_df: pd.DataFrame, ratings_df: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        '''\n",
    "        Extends users and items dataframes to match the ratings dataframe\n",
    "        '''\n",
    "        # Extend users dataframe\n",
    "        users_df = pd.merge(users_df, ratings_df[['user_id']], on='user_id', how='right')\n",
    "\n",
    "        # Extend items dataframe\n",
    "        items_df = pd.merge(items_df, ratings_df[['movie_id']], on='movie_id', how='right')\n",
    "\n",
    "        return users_df, items_df\n",
    "\n",
    "    @staticmethod\n",
    "    def multi_hot_encode(df: pd.DataFrame, col: str, delimiter='|') -> pd.DataFrame:\n",
    "        '''\n",
    "        Multi hot encodes columns in a dataframe\n",
    "        '''\n",
    "        df_ = df.copy(deep=True)\n",
    "\n",
    "        # Change Children's to Children to match the other genres\n",
    "        df_[col] = df_[col].str.replace(\"Children's\", 'Children') if col == 'genre' else df_[col]\n",
    "\n",
    "        # split genres\n",
    "        df_[col] = df_[col].str.split(delimiter)\n",
    "\n",
    "        # Create a pivot table\n",
    "        pivot_df = df_.explode(col).pivot_table(index='movie_id', columns=col, aggfunc='size', fill_value=0).reset_index()\n",
    "\n",
    "        # Merge the pivot table with the original DataFrame on 'movie_id'\n",
    "        result = pd.merge(df, pivot_df, on='movie_id', how='left')\n",
    "\n",
    "        return result.drop(columns=[col])\n",
    "\n",
    "    @staticmethod\n",
    "    def one_hot_encode(df: pd.DataFrame, cols: list[str]) -> pd.DataFrame:\n",
    "        '''\n",
    "        One hot encodes columns in a dataframe\n",
    "        '''\n",
    "        return pd.get_dummies(df, columns=cols) * 1\n",
    "\n",
    "    @staticmethod\n",
    "    def move_column(df: pd.DataFrame, col: list[str], pos: int) -> pd.DataFrame:\n",
    "        '''\n",
    "        Moves a column to a specific position in a DataFrame\n",
    "        '''\n",
    "        cols = df.columns.tolist()\n",
    "        for i in reversed(col):\n",
    "            cols.insert(pos, cols.pop(cols.index(i)))\n",
    "        return df[cols]\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess_user(user: dict, num_items: int, users: np.ndarray, weights: list[np.ndarray]=None, topk: int=3, verbose=False) -> tuple[torch.IntTensor, torch.FloatTensor, Union[list[np.ndarray], None], Union[np.ndarray, None]]:\n",
    "        '''\n",
    "        Preprocesses user data for model input\n",
    "        '''\n",
    "        if 'age' not in user or not user['age']:\n",
    "            user_ = users[user['id'] - 1]\n",
    "            user_ = np.insert(user_, 0, user['id'])\n",
    "            print(f\"User id: {user['id']} top {topk} genres: {np.array(genre)[np.argsort(user_[-18:])[-topk:][::-1]]}\") if verbose else None\n",
    "            user_ = np.tile(user_, (num_items, 1))\n",
    "            return torch.IntTensor(user_[:, 0]), torch.FloatTensor(user_[:, 1:]), None, np.array(genre)[np.argsort(user_[0, -18:])[-topk:][::-1]]\n",
    "\n",
    "        user_ = np.zeros(31, dtype=float)\n",
    "\n",
    "        user_[0] = user['id']\n",
    "\n",
    "        user_[1 if user['gender'] == 'M' else 2] = 1\n",
    "\n",
    "        user_[3 + occupation.index(user['occupation'])] = 1\n",
    "\n",
    "        # map age to bins\n",
    "        user['age'] = 1 if user['age'] < 18 else 18 if user['age'] < 25 else 25 if user['age'] < 35 else 35 if user['age'] < 45 else 45 if user['age'] < 56 else 56\n",
    "\n",
    "        user_[3 + len(occupation) + age.index(user['age'])] = 1\n",
    "\n",
    "        avg_ratings = np.zeros(len(genre), dtype=float) # 18 genres\n",
    "\n",
    "        for genre_ in user['genres']:\n",
    "            avg_ratings[genre.index(genre_)] = 1.0\n",
    "\n",
    "        user_ = np.concatenate((user_, avg_ratings))\n",
    "\n",
    "        # Get top 10 users ids of users with similar intrests (cosine similarity)\n",
    "        similar_users_ids = cosine_similarity(user_[1:].reshape(1, -1), users).argsort()[0][-10:]\n",
    "\n",
    "        # Get the mean embeddings of the top 10 similar users\n",
    "        mlp_weights = weights[0][similar_users_ids].mean(axis=0)\n",
    "        mf_weights = weights[1][similar_users_ids].mean(axis=0)\n",
    "\n",
    "        user_ = np.tile(user_, (num_items, 1))\n",
    "        return torch.IntTensor(user_[:, 0]), torch.FloatTensor(user_[:, 1:]), [mlp_weights, mf_weights], None\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess_items(items: pd.DataFrame) -> pd.DataFrame:\n",
    "        '''\n",
    "        Preprocesses items data for model input\n",
    "        '''\n",
    "        # multi hot encode genres\n",
    "        items_ = Utils.multi_hot_encode(items, 'genre')\n",
    "        items_ = Utils.extract_year(items_)\n",
    "        items_['year'] = items_['year'] / items_['year'].max()\n",
    "        items_ = items_.drop(['title'], axis=1)\n",
    "\n",
    "        return items_\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_missing_values(ratings: pd.DataFrame, items: pd.DataFrame) -> tuple[pd.DataFrame]:\n",
    "        '''\n",
    "        Removes rows with missing values from items and ratings dataframes\n",
    "        '''\n",
    "        # Get item ids with missing release dates\n",
    "        nan_item_ids = items[items[['release_date']].isna().any(axis=1)]['item_id']\n",
    "\n",
    "        # Remove movies with missing release dates\n",
    "        items.dropna(subset=['release_date'], inplace=True)\n",
    "\n",
    "        # remove ratings of missing items\n",
    "        ratings = ratings[~ratings['item_id'].isin(nan_item_ids)]\n",
    "\n",
    "        return ratings, items\n",
    "\n",
    "    @staticmethod\n",
    "    def negative_sampling(ratings: pd.DataFrame, items: pd.DataFrame, num_negatives: int) -> pd.DataFrame:\n",
    "        '''\n",
    "        Sample negative items for each user\n",
    "        '''\n",
    "        # All Movie ids\n",
    "        all_items = items['movie_id'].values\n",
    "\n",
    "        negative_samples = []\n",
    "        for user_id in ratings['user_id'].unique():\n",
    "            # Movie ids that the user has interacted with\n",
    "            pos_items = ratings[ratings['user_id'] == user_id]['movie_id'].values\n",
    "\n",
    "            # Movie ids that the user has not interacted with\n",
    "            unrated_items = np.setdiff1d(all_items, pos_items)\n",
    "\n",
    "            # Sample negative items\n",
    "            neg_items = np.random.choice(unrated_items, size=num_negatives, replace=False)\n",
    "\n",
    "            # Create negative samples\n",
    "            for item_id in neg_items:\n",
    "                negative_samples.append([user_id, item_id, 0])\n",
    "\n",
    "        negative_samples = pd.DataFrame(negative_samples, columns=['user_id', 'movie_id', 'rating'])\n",
    "\n",
    "        ratings['rating'] = [1] * ratings.shape[0]\n",
    "\n",
    "        return pd.concat([ratings, negative_samples], ignore_index=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def ndcg_hit_ratio(y_preds, X_test_users, y_true, k=10):  # تغيير هنا: قيمة افتراضية لـ k\n",
    "        '''\n",
    "        Compute NDCG and Hit Ratio\n",
    "        '''\n",
    "        if k is None:  # إضافة تحقق\n",
    "            k = 10\n",
    "\n",
    "        unique_users = np.unique(X_test_users, axis=0)\n",
    "        hits = 0\n",
    "        total_users = len(unique_users)\n",
    "\n",
    "        y_preds_padded = []\n",
    "        y_true_padded = []\n",
    "        for user in unique_users:\n",
    "            user_indices = np.where((X_test_users == user).all(axis=1))[0]\n",
    "            user_preds = y_preds[user_indices][:k].flatten()\n",
    "            user_true = y_true[user_indices][:k].flatten()\n",
    "\n",
    "            if np.any(user_true == 1):\n",
    "                hits += 1\n",
    "\n",
    "            if len(user_preds) < k:\n",
    "                user_preds = np.pad(user_preds, (0, k - len(user_preds)), mode='constant', constant_values=-1e10)\n",
    "            if len(user_true) < k:\n",
    "                user_true = np.pad(user_true, (0, k - len(user_true)), mode='constant', constant_values=0)\n",
    "\n",
    "            y_preds_padded.append(user_preds)\n",
    "            y_true_padded.append(user_true)\n",
    "\n",
    "        ndcg = ndcg_score(y_true_padded, y_preds_padded, k=k)\n",
    "        hit_ratio = hits / total_users\n",
    "        return ndcg, hit_ratio\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def pipeline(request: any, model: nn.Module, weights: list[np.ndarray], users: np.ndarray, movies: pd.DataFrame, movies_og: pd.DataFrame, ratings: pd.DataFrame, mode: str) -> tuple[list[dict], Union[np.ndarray, None]]:\n",
    "        '''\n",
    "        Pipeline for inference\n",
    "        '''\n",
    "        num_items = 300 # Number of items to retrieve\n",
    "        request = request if isinstance(request, dict) else request.model_dump()\n",
    "\n",
    "        # preprocess the old user\n",
    "        user_id, user, weights, top_n_genres = Utils.preprocess_user(\n",
    "                                        user=request,\n",
    "                                        num_items=num_items,\n",
    "                                        users=users,\n",
    "                                        weights=weights\n",
    "                                        )\n",
    "        user_id, user = user_id.to(model.device), user.to(model.device)\n",
    "\n",
    "        movies = Utils.retrieve(\n",
    "            movies=movies,\n",
    "            user=user.detach().cpu().numpy(),\n",
    "            num_genres=len(request['genres']) if request['genres'] else 3,\n",
    "            k=num_items,\n",
    "            random_state=0\n",
    "        )\n",
    "\n",
    "        movie_ids, movies = Utils.filter(\n",
    "            movies=movies,\n",
    "            ratings=ratings,\n",
    "            user_id=request['id']\n",
    "        )\n",
    "        movie_ids, movies = movie_ids.to(model.device), movies.to(model.device)\n",
    "\n",
    "        y_pred = model(\n",
    "            user_id[:len(movies)],\n",
    "            movie_ids,\n",
    "            user[:len(movies)],\n",
    "            movies,\n",
    "            weights\n",
    "        ).cpu().detach().numpy()\n",
    "\n",
    "        movies_retrieved = movies_og[movies_og['movie_id'].isin(movie_ids.cpu().numpy())].sort_values(by='movie_id', key=lambda x: pd.Categorical(x, categories=movie_ids.cpu().numpy(), ordered=True))\n",
    "\n",
    "        return Utils.order(y_pred, movies_retrieved, mode, top_k=request['top_k']).to_dict(orient='records'), top_n_genres\n",
    "\n",
    "    @staticmethod\n",
    "    def retrieve(movies: pd.DataFrame, user: np.ndarray, k: int, num_genres: int=3, random_state: int=42) -> pd.DataFrame:\n",
    "        '''\n",
    "        Retrieve top k movies based on genres based on this equation:\n",
    "        ```\n",
    "        num_movies_per_genre = k // (len(genres) + 1) # +1 for the most popular genre\n",
    "        ```\n",
    "\n",
    "        Example:\n",
    "        If k = 100 and genres = ['Action', 'Adventure', 'Animation'], then:\n",
    "        25 movies will be retrieved for each genre and 25 for the most popular genre.\n",
    "\n",
    "        movies: DataFrame containing movie information.\n",
    "        genres: List of genres to retrieve movies for.\n",
    "        k: Number of movies to retrieve.\n",
    "\n",
    "        Returns a DataFrame containing the top k movies based on the specified genres.\n",
    "        '''\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def filter(movies: pd.DataFrame, ratings: pd.DataFrame, user_id: int) -> tuple[torch.IntTensor, torch.FloatTensor]:\n",
    "        '''\n",
    "        Filter movies that the user has not interacted with, and remove duplicates\n",
    "        '''\n",
    "        # Get movie ids that the user has interacted with\n",
    "        user_movies = ratings[ratings['user_id'] == user_id]['movie_id'].values\n",
    "\n",
    "        # Filter movies that the user has not interacted with\n",
    "        movies = movies[~movies['movie_id'].isin(user_movies)]\n",
    "\n",
    "        # Remove duplicates\n",
    "        movies = movies.drop_duplicates(subset=['movie_id'])\n",
    "\n",
    "        return torch.IntTensor(movies['movie_id'].values), torch.FloatTensor(movies.drop(columns=['movie_id']).values)\n",
    "\n",
    "    @staticmethod\n",
    "    def order(y_pred: np.ndarray, movies: pd.DataFrame, mode: Literal['explicit', 'implicit'], top_k=10) -> list[dict]:\n",
    "        '''\n",
    "        Order the predictions\n",
    "        '''\n",
    "        col_name= 'predicted_rating' if mode == 'explicit' else 'predicted_score'\n",
    "        sorted_index = np.argsort(-y_pred, axis=0).reshape(-1).tolist()\n",
    "        y_pred = y_pred[sorted_index]\n",
    "        sorted_movies = movies.iloc[sorted_index]\n",
    "        sorted_movies = sorted_movies.copy()\n",
    "        sorted_movies[col_name] = y_pred if mode == 'implicit' else y_pred * 5\n",
    "        sorted_movies.reset_index(drop=True, inplace=True)\n",
    "        sorted_movies[col_name] = sorted_movies[col_name].apply(lambda x: round(x, 2))\n",
    "\n",
    "        return sorted_movies.head(top_k)\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_metrics(history: dict, title: str, figsize: tuple=(12, 4)) -> None:\n",
    "        '''\n",
    "        Plot the training and validation losses in one figure and the other metrics in another figure\n",
    "        '''\n",
    "        fig, ax = plt.subplots(1, 2, figsize=figsize)\n",
    "        ax[0].plot(history['loss'], label='Train Loss')\n",
    "        ax[0].plot(history['val_loss'], label='Validation Loss')\n",
    "        ax[0].set_title('Training and Validation Loss')\n",
    "        ax[0].set_xlabel('Epoch')\n",
    "        ax[0].set_ylabel('Loss')\n",
    "        ax[0].legend()\n",
    "\n",
    "        # Metrics plot\n",
    "        for metric, values in history.items():\n",
    "            if metric not in ['loss', 'val_loss']:\n",
    "                ax[1].plot(values, label=metric)\n",
    "\n",
    "        ax[1].set_title('Metrics')\n",
    "        ax[1].set_xlabel('Epoch')\n",
    "        ax[1].set_ylabel('Value')\n",
    "        ax[1].legend()\n",
    "        plt.suptitle(title)\n",
    "        plt.show()\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stopping to stop the training when the loss does not improve after a certain number of epochs (patience).\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=3, delta=0, verbose=False, path='checkpoint.pth') -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How many epochs to wait after the last time the validation loss improved.\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.verbose = verbose\n",
    "        self.path = path\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "\n",
    "    def __call__(self, val_loss, model: nn.Module) -> None:\n",
    "        '''\n",
    "        Call method\n",
    "        '''\n",
    "        score = -val_loss\n",
    "\n",
    "        if not self.best_score:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}') if self.verbose else None\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model: nn.Module) -> None:\n",
    "        '''\n",
    "        Save the model checkpoint\n",
    "        '''\n",
    "        print(f'Validation loss decreased ({self.best_loss:.6f} --> {val_loss:.6f}).  Saving model ...') if self.verbose else None\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.best_loss = val_loss\n",
    "\n",
    "age = [\n",
    "    1, 18, 25, 35, 45, 50, 56\n",
    "]\n",
    "\n",
    "occupation = [\n",
    "    'other', 'educator', 'artist', 'clerical', 'grad student',\n",
    "    'customer service', 'doctor', 'executive', 'farmer', 'homemaker',\n",
    "    'K-12 student', 'lawyer', 'programmer', 'retired', 'sales', 'scientist',\n",
    "    'self-employed', 'engineer', 'craftsman', 'unemployed', 'writer'\n",
    "]\n",
    "\n",
    "genre = [\n",
    "    'Action', 'Adventure', 'Animation', 'Children', 'Comedy', 'Crime',\n",
    "    'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery',\n",
    "    'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western'\n",
    "]\n",
    "\n",
    "cols_dict = {\n",
    "    'ratings': ['user_id', 'movie_id', 'rating', 'timestamp'],\n",
    "    'users': ['user_id', 'gender', 'age', 'occupation', 'zip_code'],\n",
    "    'items': ['movie_id', 'title', 'genre'],\n",
    "}\n",
    "\n",
    "css = \"\"\"\n",
    "    <style>\n",
    "        .card-container {\n",
    "            display: flex;\n",
    "            flex-direction: row;\n",
    "            justify-content: center;\n",
    "            align-items: start;\n",
    "            gap: 20px;\n",
    "            flex-wrap: wrap;\n",
    "            margin: 20px 0;\n",
    "        }\n",
    "\n",
    "        .card {\n",
    "            width: 100%;\n",
    "            max-width: 300px;\n",
    "            border: 1px solid #ddd;\n",
    "            border-radius: 8px;\n",
    "            padding: 16px;\n",
    "            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);\n",
    "            background-color: #eee;\n",
    "            transition: transform 0.2s ease-in-out;\n",
    "        }\n",
    "\n",
    "        .card:hover {\n",
    "            transform: scale(1.05);\n",
    "        }\n",
    "\n",
    "        .card-title {\n",
    "            font-size: 1.25em;\n",
    "            margin-bottom: 8px;\n",
    "            color: #333;\n",
    "        }\n",
    "\n",
    "        .card-text {\n",
    "            font-size: 1em;\n",
    "            margin-bottom: 8px;\n",
    "            color: #555;\n",
    "        }\n",
    "\n",
    "        .footer {\n",
    "        position: fixed;\n",
    "        left: 0;\n",
    "        bottom: 0;\n",
    "        width: 100%;\n",
    "        background-color: rgb(45, 38, 48);\n",
    "        color: #fff;\n",
    "        text-align: center;\n",
    "        padding: 10px;\n",
    "    }\n",
    "    </style>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "SeRfWRIpCZ6X"
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from enum import Enum\n",
    "\n",
    "class GenreEnum(str, Enum):\n",
    "    Action = \"Action\"\n",
    "    Adventure = \"Adventure\"\n",
    "    Animation = \"Animation\"\n",
    "    Childrens = \"Children\"\n",
    "    Comedy = \"Comedy\"\n",
    "    Crime = \"Crime\"\n",
    "    Documentary = \"Documentary\"\n",
    "    Drama = \"Drama\"\n",
    "    Fantasy = \"Fantasy\"\n",
    "    FilmNoir = \"Film-Noir\"\n",
    "    Horror = \"Horror\"\n",
    "    Musical = \"Musical\"\n",
    "    Mystery = \"Mystery\"\n",
    "    Romance = \"Romance\"\n",
    "    SciFi = \"Sci-Fi\"\n",
    "    Thriller = \"Thriller\"\n",
    "    War = \"War\"\n",
    "    Western = \"Western\"\n",
    "\n",
    "class OccupationEnum(str, Enum):\n",
    "    other = \"other\"\n",
    "    educator = \"educator\"\n",
    "    artist = \"artist\"\n",
    "    clerical = \"clerical\"\n",
    "    grad_student = \"grad student\"\n",
    "    customer_service = \"customer service\"\n",
    "    doctor = \"doctor\"\n",
    "    executive = \"executive\"\n",
    "    farmer = \"farmer\"\n",
    "    homemaker = \"homemaker\"\n",
    "    K_12_student = \"K-12 student\"\n",
    "    lawyer = \"lawyer\"\n",
    "    programmer = \"programmer\"\n",
    "    retired = \"retired\"\n",
    "    sales = \"sales\"\n",
    "    scientist = \"scientist\"\n",
    "    self_employed = \"self-employed\"\n",
    "    engineer = \"engineer\"\n",
    "    craftsman = \"craftsman\"\n",
    "    unemployed = \"unemployed\"\n",
    "    writer = \"writer\"\n",
    "\n",
    "class Request(BaseModel):\n",
    "    top_k: int = Field(10, ge=1, le=20, description=\"Number of recommendations\")\n",
    "    id: int = Field(..., ge=1, description=\"User\\'s ID\")\n",
    "    age: int = Field(None, ge=1, le=99, description=\"User\\'s age\")\n",
    "    occupation: OccupationEnum = Field(None, description=\"User\\'s occupation\")\n",
    "    gender: str = Field(None, pattern=\"^(M|F)$\", description=\"User\\'s gender\")\n",
    "    genres: list[GenreEnum] = Field(None, min_items=3, max_items=5, description=\"User\\'s favorite genres\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 532
    },
    "id": "YCv6qP8yyEsL",
    "outputId": "74a607b1-6519-403b-b31d-ba16b55b7e72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratings shape: (1151209, 3) \n",
      "Users shape: (1151209, 48) \n",
      "Items shape: (1151209, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  movie_id  rating\n",
       "0        1      1193       1\n",
       "1        1       661       1\n",
       "2        1       914       1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender_M</th>\n",
       "      <th>gender_F</th>\n",
       "      <th>occupation_0</th>\n",
       "      <th>occupation_1</th>\n",
       "      <th>occupation_2</th>\n",
       "      <th>occupation_3</th>\n",
       "      <th>occupation_4</th>\n",
       "      <th>occupation_5</th>\n",
       "      <th>occupation_6</th>\n",
       "      <th>occupation_7</th>\n",
       "      <th>...</th>\n",
       "      <th>freq_Fantasy</th>\n",
       "      <th>freq_Film-Noir</th>\n",
       "      <th>freq_Horror</th>\n",
       "      <th>freq_Musical</th>\n",
       "      <th>freq_Mystery</th>\n",
       "      <th>freq_Romance</th>\n",
       "      <th>freq_Sci-Fi</th>\n",
       "      <th>freq_Thriller</th>\n",
       "      <th>freq_War</th>\n",
       "      <th>freq_Western</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056604</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.264151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.113208</td>\n",
       "      <td>0.056604</td>\n",
       "      <td>0.056604</td>\n",
       "      <td>0.037736</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056604</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.264151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.113208</td>\n",
       "      <td>0.056604</td>\n",
       "      <td>0.056604</td>\n",
       "      <td>0.037736</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056604</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.264151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.113208</td>\n",
       "      <td>0.056604</td>\n",
       "      <td>0.056604</td>\n",
       "      <td>0.037736</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender_M  gender_F  occupation_0  occupation_1  occupation_2  occupation_3  \\\n",
       "0         0         1             0             0             0             0   \n",
       "1         0         1             0             0             0             0   \n",
       "2         0         1             0             0             0             0   \n",
       "\n",
       "   occupation_4  occupation_5  occupation_6  occupation_7  ...  freq_Fantasy  \\\n",
       "0             0             0             0             0  ...      0.056604   \n",
       "1             0             0             0             0  ...      0.056604   \n",
       "2             0             0             0             0  ...      0.056604   \n",
       "\n",
       "   freq_Film-Noir  freq_Horror  freq_Musical  freq_Mystery  freq_Romance  \\\n",
       "0             0.0          0.0      0.264151           0.0      0.113208   \n",
       "1             0.0          0.0      0.264151           0.0      0.113208   \n",
       "2             0.0          0.0      0.264151           0.0      0.113208   \n",
       "\n",
       "   freq_Sci-Fi  freq_Thriller  freq_War  freq_Western  \n",
       "0     0.056604       0.056604  0.037736           0.0  \n",
       "1     0.056604       0.056604  0.037736           0.0  \n",
       "2     0.056604       0.056604  0.037736           0.0  \n",
       "\n",
       "[3 rows x 48 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Action</th>\n",
       "      <th>Adventure</th>\n",
       "      <th>Animation</th>\n",
       "      <th>Children</th>\n",
       "      <th>Comedy</th>\n",
       "      <th>Crime</th>\n",
       "      <th>Documentary</th>\n",
       "      <th>Drama</th>\n",
       "      <th>Fantasy</th>\n",
       "      <th>Film-Noir</th>\n",
       "      <th>Horror</th>\n",
       "      <th>Musical</th>\n",
       "      <th>Mystery</th>\n",
       "      <th>Romance</th>\n",
       "      <th>Sci-Fi</th>\n",
       "      <th>Thriller</th>\n",
       "      <th>War</th>\n",
       "      <th>Western</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9820</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Action  Adventure  Animation  Children  Comedy  Crime  Documentary  Drama  \\\n",
       "0       0          0          0         0       0      0            0      1   \n",
       "1       0          0          1         1       0      0            0      0   \n",
       "2       0          0          0         0       0      0            0      0   \n",
       "\n",
       "   Fantasy  Film-Noir  Horror  Musical  Mystery  Romance  Sci-Fi  Thriller  \\\n",
       "0        0          0       0        0        0        0       0         0   \n",
       "1        0          0       0        1        0        0       0         0   \n",
       "2        0          0       0        1        0        1       0         0   \n",
       "\n",
       "   War  Western    year  \n",
       "0    0        0  0.9875  \n",
       "1    0        0  0.9980  \n",
       "2    0        0  0.9820  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# One-hot encode categorical features\n",
    "users_data = Utils.one_hot_encode(users_data, ['occupation', 'gender', 'age'])\n",
    "\n",
    "# Multi-hot encode genres\n",
    "items_data = Utils.multi_hot_encode(items_data, 'genre')\n",
    "\n",
    "# Features Extraction\n",
    "users_data = Utils.extract_category_freq(users_data, items_data, ratings_data)\n",
    "items_data = Utils.extract_year(items_data)\n",
    "\n",
    "# Negative Sampling for implicit feedback\n",
    "ratings_data = Utils.negative_sampling(ratings_data, items_data, num_negatives=25) # Adds 20 * 6040 rows (num_negatives * num_users)\n",
    "\n",
    "# Move gender columns to the front\n",
    "users_data = Utils.move_column(users_data, ['gender_M', 'gender_F'], 0)\n",
    "\n",
    "# Extend Users and Items data to match the number of rows in the ratings data\n",
    "users_data, items_data = Utils.extend_users_items(users_data, items_data, ratings_data)\n",
    "\n",
    "# Normalize continuous features\n",
    "items_data['year'] = items_data['year'].astype(float) / items_data['year'].max()\n",
    "\n",
    "# Drop unnecessary columns\n",
    "ratings_data = ratings_data.drop(['timestamp'], axis=1) # Implicit Interactions\n",
    "users_data = users_data.drop(['user_id', 'zip_code'], axis=1) # Demographics\n",
    "items_data = items_data.drop(['movie_id', 'title'], axis=1)\n",
    "\n",
    "print(\"Ratings shape:\", ratings_data.shape, \"\\nUsers shape:\", users_data.shape, \"\\nItems shape:\", items_data.shape)\n",
    "\n",
    "# A quick look at the data\n",
    "display(ratings_data.head(3), users_data.head(3), items_data.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oF7qKPQlHAkx",
    "outputId": "6c057e7f-d5d9-425c-bc34-2ca785b73c47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratings shape: (920967, 3) (115121, 3) (115121, 3)\n",
      "Users shape: (920967, 48) (115121, 48) (115121, 48)\n",
      "Items shape: (920967, 19) (115121, 19) (115121, 19)\n"
     ]
    }
   ],
   "source": [
    "# ratings\n",
    "y_ratings_train, y_ratings_test = train_test_split(ratings_data, test_size=0.2, shuffle=True, random_state=42)\n",
    "y_ratings_val, y_ratings_test = train_test_split(y_ratings_test, test_size=0.5, shuffle=True, random_state=42)\n",
    "\n",
    "# users\n",
    "X_users_train, X_users_test = train_test_split(users_data, test_size=0.2, shuffle=True, random_state=42)\n",
    "X_users_val, X_users_test = train_test_split(X_users_test, test_size=0.5, shuffle=True, random_state=42)\n",
    "\n",
    "# items\n",
    "X_items_train, X_items_test = train_test_split(items_data, test_size=0.2, shuffle=True, random_state=42)\n",
    "X_items_val, X_items_test = train_test_split(X_items_test, test_size=0.5, shuffle=True, random_state=42)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X_users_train, X_users_val, X_users_test = X_users_train.values, X_users_val.values, X_users_test.values\n",
    "X_items_train, X_items_val, X_items_test = X_items_train.values, X_items_val.values, X_items_test.values\n",
    "y_ratings_train, y_ratings_val, y_ratings_test = y_ratings_train.values, y_ratings_val.values, y_ratings_test.values\n",
    "\n",
    "# Shape of the data\n",
    "print(\"Ratings shape:\", y_ratings_train.shape, y_ratings_val.shape, y_ratings_test.shape)\n",
    "print(\"Users shape:\", X_users_train.shape, X_users_val.shape, X_users_test.shape)\n",
    "print(\"Items shape:\", X_items_train.shape, X_items_val.shape, X_items_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "kqdnbauU-hUd"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error, roc_auc_score\n",
    "\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "# from .utils import Utils, EarlyStopping\n",
    "from typing import Literal\n",
    "\n",
    "__model_version__ = '1.0.2'\n",
    "\n",
    "class NCF(nn.Module):\n",
    "    '''\n",
    "    Main Ranking model\n",
    "    '''\n",
    "    def __init__(self, mode: Literal['explicit', 'implicit'], num_users=6040, num_items=3952, user_dim=48, item_dim=19, num_factors=32, criterion=None, dropout=0.1, lr=1e-3, weight_decay=1e-5, verbose=False, gpu=True):\n",
    "        super(NCF, self).__init__()\n",
    "\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.mode = mode\n",
    "\n",
    "        # Embedding layers\n",
    "        self.user_embedding_mlp = nn.Embedding(num_users+1,  num_factors)\n",
    "        self.item_embedding_mlp = nn.Embedding(num_items+1,  num_factors)\n",
    "\n",
    "        self.user_embedding_mf = nn.Embedding(num_users+1, num_factors)\n",
    "        self.item_embedding_mf = nn.Embedding(num_items+1, num_factors)\n",
    "\n",
    "        # Fully connected layers for user/item features\n",
    "        self.user_features = nn.Sequential(\n",
    "            nn.Linear(user_dim, num_factors*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_factors*2, num_factors),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.item_features = nn.Sequential(\n",
    "            nn.Linear(item_dim, num_factors*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_factors*2, num_factors),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # MLP\n",
    "        self.MLP = nn.Sequential(\n",
    "            nn.Linear(4 * num_factors, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, num_factors),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # NeuMF layer\n",
    "        self.neu_mf = nn.Linear(2 * num_factors, 1)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr, weight_decay=weight_decay) # weight_decay is L2 regularization\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() and gpu else 'cpu')\n",
    "\n",
    "        self.__init_weights()\n",
    "\n",
    "        self.to(self.device)\n",
    "\n",
    "        # Print the model architecture\n",
    "        print(self, '\\nRunning on: ', self.device) if verbose else None\n",
    "        print(f'Number of parameters: {self.params_count():,}') if verbose else None\n",
    "\n",
    "    def __init_data(self, input, y=None):\n",
    "        '''\n",
    "        Initialize the data\n",
    "        '''\n",
    "        X_user_id, X_item_id, X_user, X_item = input\n",
    "\n",
    "        X_user = torch.FloatTensor(X_user).to(self.device, non_blocking=True)\n",
    "        X_item = torch.FloatTensor(X_item).to(self.device, non_blocking=True)\n",
    "        X_user_id = torch.IntTensor(X_user_id).to(self.device, non_blocking=True)\n",
    "        X_item_id = torch.IntTensor(X_item_id).to(self.device, non_blocking=True)\n",
    "        y = torch.FloatTensor(y).to(self.device, non_blocking=True) if y is not None else None\n",
    "\n",
    "\n",
    "        return X_user_id, X_item_id, X_user, X_item, y\n",
    "\n",
    "    def __init_weights(self) -> None:\n",
    "        '''\n",
    "        Initialize the weights\n",
    "        '''\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Embedding):\n",
    "                nn.init.normal_(m.weight, mean=0, std=0.01) # Normal initialization with mean 0 and standard deviation 0.01\n",
    "\n",
    "        for m in self.MLP.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_uniform_(m.weight, mode='fan_in', nonlinearity='relu') # He initialization, fan_in preserves the magnitude of the variance of the weights in the forward pass\n",
    "                m.bias.data.fill_(0.01)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.neu_mf.weight, gain=nn.init.calculate_gain('sigmoid')) # Glorot initialization\n",
    "\n",
    "    def forward(self, user_id: torch.IntTensor, item_id: torch.IntTensor, user_features: torch.FloatTensor, item_features: torch.FloatTensor, weights=None)-> torch.Tensor:\n",
    "        '''\n",
    "        Forward pass\n",
    "        '''\n",
    "        # Embedding Layers\n",
    "        if not weights:\n",
    "            user_embedding_mlp = self.user_embedding_mlp(user_id) # (batch_size, num_factors)\n",
    "            user_embedding_mf = self.user_embedding_mf(user_id) # (batch_size, num_factors)\n",
    "        else: # Explicit embeddings for OOV users\n",
    "            user_embedding_mlp = torch.tensor(weights[0], device=self.device).repeat(len(user_id), 1) # (batch_size, num_factors)\n",
    "            user_embedding_mf = torch.tensor(weights[1], device=self.device).repeat(len(user_id), 1) # (batch_size, num_factors)\n",
    "\n",
    "        item_embedding_mlp = self.item_embedding_mlp(item_id) # (batch_size, num_factors)\n",
    "        item_embedding_mf = self.item_embedding_mf(item_id) # (batch_size, num_factors)\n",
    "\n",
    "        # User and Item Features\n",
    "        user_features = self.user_features(user_features) # (batch_size, num_factors)\n",
    "        item_features = self.item_features(item_features) # (batch_size, num_factors)\n",
    "\n",
    "        # Concatenate the embeddings and features\n",
    "        user_embedding_mlp = torch.cat([user_embedding_mlp, user_features], dim=-1) # (batch_size, 2 * num_factors)\n",
    "        item_embedding_mlp = torch.cat([item_embedding_mlp, item_features], dim=-1) # (batch_size, 2 * num_factors)\n",
    "\n",
    "        # MLP Branch\n",
    "        mlp_input = torch.cat([user_embedding_mlp, item_embedding_mlp], dim=-1) # (batch_size, 4 * num_factors)\n",
    "        mlp_output = self.MLP(mlp_input) # (batch_size, num_factors)\n",
    "\n",
    "        # GMF Branch\n",
    "        mf_output = torch.mul(user_embedding_mf, item_embedding_mf) # (batch_size, num_factors)\n",
    "\n",
    "        # NeuMF Layer\n",
    "        neu_mf_input = torch.cat([mlp_output, mf_output], dim=-1) # (batch_size, 2 * num_factors)\n",
    "        neu_mf = self.neu_mf(neu_mf_input) # (batch_size, 1)\n",
    "\n",
    "        return self.sigmoid(neu_mf).flatten()\n",
    "    def get_user_embedding(self, user_id):\n",
    "        self.eval()  \n",
    "        with torch.no_grad():\n",
    "            user_id_tensor = torch.tensor([user_id], dtype=torch.long, device=self.device)\n",
    "            user_embedding = self.user_embedding_mf(user_id_tensor)\n",
    "            return user_embedding.squeeze(0).detach().cpu().numpy()\n",
    "            \n",
    "    def get_item_embedding(self, item_id):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            item_id_tensor = torch.tensor([item_id], dtype=torch.long, device=self.device)\n",
    "            item_embedding = self.item_embedding_mf(item_id_tensor)\n",
    "            return item_embedding.squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "    def fit(self, X: list[np.ndarray], y: np.ndarray, epochs: int, batch_size: int, X_val: list[np.ndarray] = None, y_val: np.ndarray = None, k:int = 10, scheduler: torch.optim.lr_scheduler = None, early_stopping: EarlyStopping = None) -> dict:\n",
    "        '''\n",
    "        Train the model\n",
    "\n",
    "        params:\n",
    "        X: list[np.ndarray] - [user_ids, item_ids, user_features, item_features]\n",
    "        y: np.ndarray - Target values\n",
    "        epochs: int - Number of epochs\n",
    "        batch_size: int - Batch size\n",
    "        k: int - Number of top-k items to consider\n",
    "        X_val: list[np.ndarray] - List of users and items features for validation\n",
    "        y_val: np.ndarray - Target values for validation\n",
    "\n",
    "        returns:\n",
    "        list[float] - Loss values for plotting\n",
    "        '''\n",
    "        X_user_id, X_item_id, X_user, X_item, y = self.__init_data(X, y)\n",
    "\n",
    "        dataset = TensorDataset(X_user_id, X_item_id, X_user, X_item, y)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        self.train()\n",
    "\n",
    "        # self.__init_weights() # Reinitialize the weights to prevent using the weights from the previous training session\n",
    "\n",
    "        losses = []\n",
    "        all_metrics = []\n",
    "        lrs = []\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0.0\n",
    "            for i, batch in enumerate(dataloader):\n",
    "                X_user_id, X_item_id, X_user, X_item, y = batch\n",
    "\n",
    "                self.optimizer.zero_grad() # Zero the gradients\n",
    "                output = self(X_user_id, X_item_id, X_user, X_item) # Forward pass\n",
    "                loss = self.criterion(output, y) # Compute the loss\n",
    "                loss.backward() # Backward pass\n",
    "                self.optimizer.step() # Update the weights\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                print(f'Epoch {epoch+1}/{epochs}') if i == 0 else None\n",
    "                print(f'{i+1}/{len(dataloader)} - loss: {(total_loss / (i+1)):.4f}', end='\\r' if i + 1 < len(dataloader) else ' ')\n",
    "\n",
    "            losses.append(total_loss / (i + 1))\n",
    "            # Evaluate the model\n",
    "            if X_val is not None and y_val is not None:\n",
    "                self.eval()\n",
    "                metrics = self.evaluate(X_val, y_val, batch_size, k)\n",
    "\n",
    "                if scheduler:\n",
    "                    scheduler.step(metrics[0])\n",
    "                    lrs.append(scheduler.get_last_lr()[0])\n",
    "\n",
    "                if self.mode == 'explicit':\n",
    "                    all_metrics.append([*metrics])\n",
    "                    print(f'- Val Loss: {metrics[0]:.4f} - R2: {metrics[1]:.4f} - MAE: {metrics[2]:.4f} - MSE: {metrics[3]:.4f} - RMSE: {metrics[4]:.4f} - lr: {lrs[-1]}')\n",
    "                else:\n",
    "                    all_metrics.append([*metrics])\n",
    "                    print(f'- Val Loss: {metrics[0]:.4f} - NDCG: {metrics[1]:.4f} - HR: {metrics[2]:.4f} - ROC-AUC: {metrics[3]:.4f} - lr: {lrs[-1]}')\n",
    "\n",
    "                if early_stopping:\n",
    "                    early_stopping(metrics[0], self)\n",
    "                    if early_stopping.early_stop:\n",
    "                        print(\"Early stopping\")\n",
    "                        break\n",
    "\n",
    "                self.train()\n",
    "            else:\n",
    "                print()\n",
    "\n",
    "        if self.mode == 'explicit':\n",
    "            history = {\n",
    "                'loss': losses,\n",
    "                'val_loss': [m[0] for m in all_metrics],\n",
    "                'r2': [m[1] for m in all_metrics],\n",
    "                'mae': [m[2] for m in all_metrics],\n",
    "                'mse': [m[3] for m in all_metrics],\n",
    "                'rmse': [m[4] for m in all_metrics],\n",
    "                'lr': lrs\n",
    "            }\n",
    "        else:\n",
    "            history = {\n",
    "                'loss': losses,\n",
    "                'val_loss': [m[0] for m in all_metrics],\n",
    "                'ndcg': [m[1] for m in all_metrics],\n",
    "                'hr': [m[2] for m in all_metrics],\n",
    "                'roc_auc': [m[3] for m in all_metrics],\n",
    "                'lr': lrs\n",
    "            }\n",
    "\n",
    "        return history\n",
    "\n",
    "    def predict(self, user_id: torch.IntTensor, item_id:torch.IntTensor, user: torch.FloatTensor, items: torch.FloatTensor)-> torch.Tensor:\n",
    "        '''\n",
    "        Alias for forward method, Initializes data first then calls forward method\n",
    "        '''\n",
    "        user_id, item_id, user_input, item_input, _ = self.__init_data([user_id, item_id, user, items])\n",
    "        return self(user_id, item_id, user_input, item_input)\n",
    "\n",
    "    def evaluate(self, X_val: list, y_val: np.ndarray, batch_size: int, k: int = None) -> tuple[float]:\n",
    "        '''\n",
    "        Evaluate the model\n",
    "\n",
    "        params:\n",
    "        X_val: list - List of users and items features\n",
    "        y_val: np.ndarray - Target values\n",
    "        batch_size: int - Batch size\n",
    "        type: Literal['explicit', 'implicit'] - Type of model (explicit or implicit)\n",
    "        k: int - Number of top-k items to consider\n",
    "        thresh: float - Threshold for hit rate\n",
    "\n",
    "        returns:\n",
    "        tuple[float] - Val loss, MAE, R2, RMSE, MSE for explicit model\n",
    "        tuple[float] - Val loss, NDCG, HR, AUC for implicit model\n",
    "        '''\n",
    "        user_id, item_id, user_input, item_input, target = self.__init_data(X_val, y_val)\n",
    "\n",
    "        dataset = TensorDataset(user_id, item_id, user_input, item_input, target)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "        avg_loss, y_preds = 0.0, []\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(dataloader):\n",
    "                user_id, item_id, user_input, item_input, y = batch\n",
    "\n",
    "                # Predictions\n",
    "                preds = self(user_id, item_id, user_input, item_input)\n",
    "\n",
    "                # Append to y_preds\n",
    "                y_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "                # Compute loss\n",
    "                loss = self.criterion(preds, y)\n",
    "                avg_loss += loss.item()\n",
    "\n",
    "        avg_loss /= (i + 1)\n",
    "\n",
    "        target = target.cpu().numpy()\n",
    "        y_preds = np.array(y_preds).reshape(-1)\n",
    "\n",
    "        # Compute additional evaluation metrics\n",
    "        if self.mode == 'explicit':\n",
    "            r2 = r2_score(target, y_preds)\n",
    "            mae = mean_absolute_error(target, y_preds)\n",
    "            mse = mean_squared_error(target, y_preds)\n",
    "            rmse = sqrt(mse)\n",
    "\n",
    "            return avg_loss, r2, mae, mse, rmse\n",
    "        else:\n",
    "            ndcg, hr = Utils.ndcg_hit_ratio(y_preds, X_val[2], y_val, k)\n",
    "            roc_auc = roc_auc_score(target, y_preds)\n",
    "\n",
    "            return avg_loss, ndcg, hr, roc_auc\n",
    "\n",
    "    def params_count(self)-> int:\n",
    "        '''\n",
    "        Count the number of parameters in the model\n",
    "        '''\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "    def save_weights(self, path)-> None:\n",
    "        '''\n",
    "        Save the model weights\n",
    "        '''\n",
    "        torch.save(self.state_dict(), path)\n",
    "\n",
    "    def load_weights(self, path, eval=True)-> None:\n",
    "        '''\n",
    "        Load the model weights\n",
    "\n",
    "        Note: Set eval to True if you want to set the model to evaluation mode (dropout layers will be disabled)\n",
    "        '''\n",
    "        self.load_state_dict(torch.load(path, map_location=self.device))\n",
    "        self.eval() if eval else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dwYHgCPXoEFn",
    "outputId": "43f86014-ad7c-425d-bf6a-3b6e02c46516"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NCF(\n",
      "  (user_embedding_mlp): Embedding(6042, 32)\n",
      "  (item_embedding_mlp): Embedding(3954, 32)\n",
      "  (user_embedding_mf): Embedding(6042, 32)\n",
      "  (item_embedding_mf): Embedding(3954, 32)\n",
      "  (user_features): Sequential(\n",
      "    (0): Linear(in_features=48, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (item_features): Sequential(\n",
      "    (0): Linear(in_features=19, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (MLP): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.1, inplace=False)\n",
      "    (6): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Dropout(p=0.1, inplace=False)\n",
      "    (9): Linear(in_features=128, out_features=32, bias=True)\n",
      "    (10): ReLU()\n",
      "  )\n",
      "  (neu_mf): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      "  (criterion): BCELoss()\n",
      ") \n",
      "Running on:  cpu\n",
      "Number of parameters: 882,785\n"
     ]
    }
   ],
   "source": [
    "\n",
    "user_dim = users_data.shape[1] # 48\n",
    "item_dim = items_data.shape[1] # 19\n",
    "num_users = int(users_data_og['user_id'].max()) + 1  # +1 لأن الفهرس يبدأ من 0\n",
    "num_items = int(items_data_og['movie_id'].max()) + 1\n",
    "\n",
    "# num_users = users_data_og['user_id'].max()\n",
    "# num_items = items_data_og['movie_id'].max()\n",
    "\n",
    "embedding_dim = 32 # number of latent factors\n",
    "\n",
    "model = NCF(\n",
    "    num_users=num_users,\n",
    "    num_items=num_items,\n",
    "    user_dim=user_dim,\n",
    "    item_dim=item_dim,\n",
    "    num_factors=embedding_dim,\n",
    "    mode='implicit',\n",
    "    criterion=torch.nn.BCELoss(), # BCELoss since output layer has sigmoid applied to it, otherwise BCEWithLogitsLoss()\n",
    "    dropout=0.1,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-5, # L2 regularization\n",
    "    verbose=True,\n",
    "    gpu=True\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(patience=2, delta=0.001, path='C:/Users/lenovo/Desktop/LinUCB-HybridRecommender/implicit.pth') # early stops after 2 consecutive epochs with minor loss decrease/increase\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(model.optimizer, mode='min', factor=0.1, patience=0) # reduces learning rate by factor of 0.1 when no improvement is seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 68874 association rules\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "def generate_association_rules(user_item_matrix, min_support=0.01, min_confidence=0.2, max_len=2):\n",
    "    \"\"\"\n",
    "    توليد قواعد ارتباطية من مصفوفة تفاعلات المستخدمين والعناصر\n",
    "    \n",
    "    Args:\n",
    "        user_item_matrix: مصفوفة تفاعلات (مستخدمين × عناصر) - يجب أن تكون pandas.DataFrame\n",
    "        min_support: الحد الأدنى للدعم\n",
    "        min_confidence: الحد الأدنى للثقة\n",
    "        max_len: الطول الأقصى للقاعدة\n",
    "    \n",
    "    Returns:\n",
    "        list: قواعد الارتباط [(antecedents, consequents, confidence), ...]\n",
    "    \"\"\"\n",
    "    # تحويل المصفوفة إلى DataFrame إذا لم تكن كذلك\n",
    "    if not isinstance(user_item_matrix, pd.DataFrame):\n",
    "        user_item_matrix = pd.DataFrame.sparse.from_spmatrix(user_item_matrix)\n",
    "    \n",
    "    # تحويل القيم إلى بوليان (True/False)\n",
    "    binary_matrix = user_item_matrix.astype(bool)\n",
    "    \n",
    "    # إيجاد العناصر المتكررة\n",
    "    frequent_itemsets = apriori(binary_matrix, \n",
    "                              min_support=min_support, \n",
    "                              use_colnames=True,\n",
    "                              max_len=max_len)\n",
    "    \n",
    "    # توليد القواعد الترابطية\n",
    "    rules = association_rules(frequent_itemsets, \n",
    "                            metric=\"confidence\", \n",
    "                            min_threshold=min_confidence)\n",
    "    \n",
    "    # تحويل النتائج إلى الشكل المطلوب\n",
    "    formatted_rules = []\n",
    "    for _, rule in rules.iterrows():\n",
    "        antecedents = list(rule['antecedents'])\n",
    "        consequents = list(rule['consequents'])\n",
    "        confidence = rule['confidence']\n",
    "        formatted_rules.append((antecedents, consequents, confidence))\n",
    "    \n",
    "    return formatted_rules\n",
    "\n",
    "# تعديل دالة إنشاء المصفوفة لتعيد DataFrame بدلاً من csr_matrix\n",
    "def create_user_item_matrix(ratings_df, num_users, num_items):\n",
    "    \"\"\"\n",
    "    إنشاء مصفوفة تفاعلات مستخدمين-عناصر كـ DataFrame\n",
    "    \"\"\"\n",
    "    sparse_matrix = csr_matrix((ratings_df['rating'], \n",
    "                             (ratings_df['user_id'], ratings_df['movie_id'])),\n",
    "                            shape=(num_users, num_items))\n",
    "    \n",
    "    return pd.DataFrame.sparse.from_spmatrix(sparse_matrix)\n",
    "\n",
    "user_item_matrix = create_user_item_matrix(ratings_data, num_users, num_items)\n",
    "rules = generate_association_rules(user_item_matrix, min_support=0.05, min_confidence=0.4, max_len=2)\n",
    "print(f\"Generated {len(rules)} association rules\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import ndcg_score\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "ncf_construction_params = {\n",
    "    'num_users': int(users_data_og['user_id'].max()) + 1,\n",
    "    'num_items': int(items_data_og['movie_id'].max()) + 1,\n",
    "    'user_dim': users_data.shape[1],  # 48\n",
    "    'item_dim': items_data.shape[1],  # 19\n",
    "    'num_factors': 32,  # embedding_dim\n",
    "    'mode': 'implicit',\n",
    "    'criterion': torch.nn.BCELoss(),\n",
    "    'dropout': 0.1,\n",
    "    'lr': 1e-3,\n",
    "    'weight_decay': 1e-5,\n",
    "    'verbose': True,\n",
    "    'gpu': True\n",
    "}\n",
    "\n",
    "# تعديل فئة ModelTrainer لإنشاء scheduler بشكل صحيح\n",
    "class ModelTrainer:\n",
    "    def __init__(self, model_class, model_path='saved_models'):\n",
    "        self.model_class = model_class\n",
    "        self.model_path = model_path\n",
    "        os.makedirs(model_path, exist_ok=True)\n",
    "    \n",
    "    def train(self, train_data, val_data, X_users_train, X_items_train, X_users_val, X_items_val, \n",
    "              epochs=12, batch_size=2048, patience=2, delta=0.001, **model_kwargs):\n",
    "        # إنشاء النموذج\n",
    "        model = self.model_class(**model_kwargs)\n",
    "        \n",
    "        # إنشاء early stopping مع مسار الحفظ\n",
    "        early_stopping = EarlyStopping(\n",
    "            patience=patience,\n",
    "            delta=delta,\n",
    "            path=os.path.join(self.model_path, 'best_model.pth')\n",
    "        )\n",
    "        \n",
    "        # إنشاء scheduler بعد إنشاء النموذج\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer=model.optimizer,\n",
    "            mode='min',\n",
    "            factor=0.1,\n",
    "            patience=0\n",
    "        )\n",
    "        \n",
    "        # التدريب\n",
    "        history = model.fit(\n",
    "            X=[train_data[:, 0], train_data[:, 1], X_users_train, X_items_train],\n",
    "            y=train_data[:, 2],\n",
    "            X_val=[val_data[:, 0], val_data[:, 1], X_users_val, X_items_val],\n",
    "            y_val=val_data[:, 2],\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            early_stopping=early_stopping,\n",
    "            scheduler=scheduler\n",
    "        )\n",
    "        \n",
    "        # حفظ النموذج النهائي\n",
    "        save_path = os.path.join(self.model_path, f'{self.model_class.__name__}_final.pth')\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(f\"Model saved to {save_path}\")\n",
    "        return model, history\n",
    "\n",
    "\n",
    "class HybridRecommenderLR:\n",
    "    def __init__(self, neu_mf_model, rules, user_item_matrix, ratings_df, movies_df):\n",
    "        self.neu_mf = neu_mf_model\n",
    "        self.rules = rules\n",
    "        self.user_item_matrix = user_item_matrix\n",
    "        self.ratings_df = ratings_df\n",
    "        self.movies_df = movies_df\n",
    "        self.all_item_ids = ratings_df['movie_id'].unique()\n",
    "        self._prepare_knn()\n",
    "        self._validate_data()\n",
    "        self.lr_model = None\n",
    "        self.feature_scaler = StandardScaler()\n",
    "        \n",
    "    def _validate_data(self):\n",
    "        required_movie_cols = {'movieId', 'title', 'genres'}\n",
    "        if not required_movie_cols.issubset(self.movies_df.columns):\n",
    "            missing = required_movie_cols - set(self.movies_df.columns)\n",
    "            raise ValueError(f\"Missing movie columns: {missing}\")\n",
    "\n",
    "    def _prepare_knn(self):\n",
    "        self.item_ids = np.array(list(self.ratings_df['movie_id'].unique()))\n",
    "        self.item_embeddings = {}\n",
    "        valid_items = []\n",
    "        embeddings = []\n",
    "\n",
    "        for item in self.item_ids:\n",
    "            try:\n",
    "                emb = self.neu_mf.get_item_embedding(item)\n",
    "                self.item_embeddings[item] = emb\n",
    "                valid_items.append(item)\n",
    "                embeddings.append(emb)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error getting embedding for item {item}: {str(e)}\")\n",
    "\n",
    "        if not valid_items:\n",
    "            raise ValueError(\"No valid items for KNN initialization\")\n",
    "\n",
    "        self.item_ids = np.array(valid_items)\n",
    "        embeddings = np.array(embeddings)\n",
    "\n",
    "        if embeddings.shape[1] > 50:\n",
    "            from sklearn.decomposition import PCA\n",
    "            pca = PCA(n_components=min(32, embeddings.shape[1]))\n",
    "            embeddings = pca.fit_transform(embeddings)\n",
    "            print(f\"Reduced dimensions from {embeddings.shape[1]} to 32 using PCA\")\n",
    "\n",
    "        self.knn_model = NearestNeighbors(n_neighbors=50, metric='cosine', algorithm='brute')\n",
    "        self.knn_model.fit(embeddings)\n",
    "\n",
    "        for i, item in enumerate(valid_items):\n",
    "            self.item_embeddings[item] = embeddings[i]\n",
    "    \n",
    "    def prepare_training_data(self, train_interactions, sample_negatives=3):\n",
    "        X = []\n",
    "        y = []\n",
    "        pos_interactions = set((u, i) for u, i in train_interactions[:, :2])\n",
    "        \n",
    "        for user_id, item_id in tqdm(train_interactions[:, :2], desc=\"Preparing training data\"):\n",
    "            features = self._extract_features(user_id, item_id)\n",
    "            X.append(features)\n",
    "            y.append(1)\n",
    "            \n",
    "            user_items = set(self.ratings_df[self.ratings_df['user_id'] == user_id]['movie_id'])\n",
    "            all_items = set(self.all_item_ids)\n",
    "            negative_candidates = list(all_items - user_items)\n",
    "            \n",
    "            for _ in range(sample_negatives):\n",
    "                neg_item = np.random.choice(negative_candidates)\n",
    "                features = self._extract_features(user_id, neg_item)\n",
    "                X.append(features)\n",
    "                y.append(0)\n",
    "        \n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    def _extract_features(self, user_id, item_id):\n",
    "        features = []\n",
    "        \n",
    "        # NeuMF features\n",
    "        try:\n",
    "            user_tensor = torch.tensor([user_id], dtype=torch.long)\n",
    "            item_tensor = torch.tensor([item_id], dtype=torch.long)\n",
    "            user_features = torch.tensor([X_users_train[user_id]], dtype=torch.float32)\n",
    "            item_features = torch.tensor([X_items_train[item_id]], dtype=torch.float32)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                ncf_score = self.neu_mf(user_tensor, item_tensor, user_features, item_features).item()\n",
    "            features.append(ncf_score)\n",
    "        except:\n",
    "            features.append(0)\n",
    "        \n",
    "        # Rule-based features\n",
    "        rule_score = 0\n",
    "        for rule in self.rules:\n",
    "            antecedents, consequents, confidence = rule\n",
    "            if item_id in consequents:\n",
    "                rule_score += confidence\n",
    "        features.append(rule_score)\n",
    "        \n",
    "        # KNN features\n",
    "        knn_score = 0\n",
    "        if item_id in self.item_embeddings:\n",
    "            user_items = set(self.ratings_df[self.ratings_df['user_id'] == user_id]['movie_id'])\n",
    "            similarities = []\n",
    "            for ui in user_items:\n",
    "                if ui in self.item_embeddings:\n",
    "                    sim = 1 - np.linalg.norm(self.item_embeddings[item_id] - self.item_embeddings[ui])\n",
    "                    similarities.append(sim)\n",
    "            if similarities:\n",
    "                knn_score = np.mean(similarities)\n",
    "        features.append(knn_score)\n",
    "        \n",
    "        # Additional features\n",
    "        item_popularity = self.ratings_df['movie_id'].value_counts().get(item_id, 0)\n",
    "        features.append(item_popularity)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def train_logistic_regression(self, X_train, y_train):\n",
    "        self.lr_model = make_pipeline(\n",
    "            StandardScaler(),\n",
    "            LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "        )\n",
    "        self.lr_model.fit(X_train, y_train)\n",
    "        print(\"Logistic Regression model trained successfully\")\n",
    "        print(f\"Feature weights: {self.lr_model.steps[1][1].coef_}\")\n",
    "    \n",
    "    def hybrid_recommend_lr(self, user_id, item_id=None, k=10):\n",
    "        if not item_id:\n",
    "            rec_items, _ = self.get_neu_mf_recommendations(user_id, k*2)\n",
    "        else:\n",
    "            neu_mf_items, _ = self.get_neu_mf_recommendations(user_id, k)\n",
    "            rule_items = [r[0] for r in self.get_rule_based_recommendations(item_id)]\n",
    "            knn_items = [r[0] for r in self.get_knn_recommendations(item_id, k//2)]\n",
    "            rec_items = list(set(neu_mf_items + rule_items + knn_items))\n",
    "        \n",
    "        if self.lr_model is None:\n",
    "            print(\"Warning: LR model not trained, using manual hybrid\")\n",
    "            return self.hybrid_recommend(user_id, item_id, k)\n",
    "        \n",
    "        X_pred = []\n",
    "        valid_items = []\n",
    "        \n",
    "        for item in rec_items:\n",
    "            try:\n",
    "                features = self._extract_features(user_id, item)\n",
    "                X_pred.append(features)\n",
    "                valid_items.append(item)\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        if not valid_items:\n",
    "            return []\n",
    "        \n",
    "        X_pred = np.array(X_pred)\n",
    "        scores = self.lr_model.predict_proba(X_pred)[:, 1]\n",
    "        ranked_items = sorted(zip(valid_items, scores), key=lambda x: x[1], reverse=True)[:k]\n",
    "        \n",
    "        recommendations = []\n",
    "        for item_id, score in ranked_items:\n",
    "            try:\n",
    "                movie = self.movies_df[self.movies_df['movieId'] == item_id].iloc[0]\n",
    "                explanation = []\n",
    "                \n",
    "                if score > 0.7:\n",
    "                    explanation.append(f\"Highly Recommended (Confidence: {score:.2f})\")\n",
    "                elif score > 0.4:\n",
    "                    explanation.append(f\"Recommended (Confidence: {score:.2f})\")\n",
    "                else:\n",
    "                    explanation.append(f\"Suggested (Confidence: {score:.2f})\")\n",
    "                \n",
    "                genres = movie['genres'].split('|')[:3]\n",
    "                \n",
    "                recommendations.append({\n",
    "                    'movieId': item_id,\n",
    "                    'title': movie['title'],\n",
    "                    'genres': ', '.join(genres),\n",
    "                    'score': round(score, 4),\n",
    "                    'explanation': \" - \".join(explanation)\n",
    "                })\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "class AdvancedEvaluator:\n",
    "    @staticmethod\n",
    "    def evaluate_model(recommender, test_data, train_data, k=10, model_type='hybrid_lr', sample_size=None):\n",
    "        test_df = pd.DataFrame(test_data[:, :2], columns=['user_id', 'movie_id'])\n",
    "        train_df = pd.DataFrame(train_data[:, :2], columns=['user_id', 'movie_id'])\n",
    "        \n",
    "        if sample_size and sample_size < len(test_df['user_id'].unique()):\n",
    "            users_sample = np.random.choice(test_df['user_id'].unique(), size=sample_size, replace=False)\n",
    "            test_df = test_df[test_df['user_id'].isin(users_sample)]\n",
    "        \n",
    "        train_user_items = train_df.groupby('user_id')['movie_id'].apply(set).to_dict()\n",
    "        test_user_items = test_df.groupby('user_id')['movie_id'].apply(set).to_dict()\n",
    "        \n",
    "        metrics = {\n",
    "            'precision': [], 'recall': [], 'hit': 0, 'ndcg': [],\n",
    "            'map': [], 'users_processed': 0, 'diversity': [], 'novelty': []\n",
    "        }\n",
    "        \n",
    "        item_popularity = train_df['movie_id'].value_counts().to_dict()\n",
    "        max_popularity = max(item_popularity.values()) if item_popularity else 1\n",
    "        \n",
    "        for user_id, test_items in tqdm(test_user_items.items(), desc=f'Evaluating {model_type} (k={k})'):\n",
    "            try:\n",
    "                recs = AdvancedEvaluator._get_recommendations(recommender, user_id, test_items, k, model_type)\n",
    "                seen_items = train_user_items.get(user_id, set())\n",
    "                recs = [item for item in recs if item not in seen_items][:k]\n",
    "                \n",
    "                if not recs:\n",
    "                    continue\n",
    "                \n",
    "                relevant = len(test_items & set(recs))\n",
    "                metrics['precision'].append(relevant / len(recs))\n",
    "                metrics['recall'].append(relevant / len(test_items) if test_items else 0)\n",
    "                \n",
    "                if relevant > 0:\n",
    "                    metrics['hit'] += 1\n",
    "                \n",
    "                y_true = [1 if item in test_items else 0 for item in recs]\n",
    "                if len(y_true) > 1:\n",
    "                    metrics['ndcg'].append(ndcg_score([y_true], [range(len(y_true), 0, -1)], k=k))\n",
    "                else:\n",
    "                    metrics['ndcg'].append(1.0 if y_true[0] == 1 else 0.0)\n",
    "                \n",
    "                precisions = []\n",
    "                hit_count = 0\n",
    "                for i, item in enumerate(recs, 1):\n",
    "                    if item in test_items:\n",
    "                        hit_count += 1\n",
    "                        precisions.append(hit_count / i)\n",
    "                metrics['map'].append(np.mean(precisions) if precisions else 0.0)\n",
    "                \n",
    "                unique_genres = set()\n",
    "                for item in recs:\n",
    "                    try:\n",
    "                        genres = recommender.movies_df[recommender.movies_df['movieId'] == item]['genres'].iloc[0].split('|')\n",
    "                        unique_genres.update(genres)\n",
    "                    except:\n",
    "                        continue\n",
    "                metrics['diversity'].append(len(unique_genres) / len(recs) if recs else 0)\n",
    "                \n",
    "                novelty_scores = []\n",
    "                for item in recs:\n",
    "                    popularity = item_popularity.get(item, 0) / max_popularity\n",
    "                    novelty_scores.append(-np.log(popularity + 1e-9))\n",
    "                metrics['novelty'].append(np.mean(novelty_scores) if novelty_scores else 0)\n",
    "                \n",
    "                metrics['users_processed'] += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error evaluating user {user_id}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        return {\n",
    "            'model_type': model_type,\n",
    "            'k': k,\n",
    "            'precision@k': np.mean(metrics['precision']) if metrics['precision'] else 0,\n",
    "            'recall@k': np.mean(metrics['recall']) if metrics['recall'] else 0,\n",
    "            'hit_ratio': metrics['hit'] / metrics['users_processed'] if metrics['users_processed'] > 0 else 0,\n",
    "            'ndcg@k': np.mean(metrics['ndcg']) if metrics['ndcg'] else 0,\n",
    "            'map@k': np.mean(metrics['map']) if metrics['map'] else 0,\n",
    "            'diversity': np.mean(metrics['diversity']) if metrics['diversity'] else 0,\n",
    "            'novelty': np.mean(metrics['novelty']) if metrics['novelty'] else 0,\n",
    "            'users_evaluated': metrics['users_processed']\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_recommendations(recommender, user_id, test_items, k, model_type):\n",
    "        if model_type == 'hybrid_lr':\n",
    "            first_item = next(iter(test_items)) if test_items else None\n",
    "            return [r['movieId'] for r in recommender.hybrid_recommend_lr(user_id=int(user_id), item_id=first_item, k=k)]\n",
    "        elif model_type == 'hybrid':\n",
    "            first_item = next(iter(test_items)) if test_items else None\n",
    "            return [r['movieId'] for r in recommender.hybrid_recommend(user_id=int(user_id), item_id=first_item, k=k)]\n",
    "        elif model_type == 'neumf':\n",
    "            rec_items, _ = recommender.get_neu_mf_recommendations(int(user_id), k)\n",
    "            return rec_items\n",
    "        elif model_type == 'rule':\n",
    "            if test_items:\n",
    "                first_item = next(iter(test_items))\n",
    "                rule_recs = recommender.get_rule_based_recommendations(first_item)\n",
    "                return [r[0] for r in rule_recs][:k]\n",
    "            return []\n",
    "        elif model_type == 'knn':\n",
    "            if test_items:\n",
    "                first_item = next(iter(test_items))\n",
    "                knn_recs = recommender.get_knn_recommendations(first_item, k//2)\n",
    "                return [r[0] for r in knn_recs][:k]\n",
    "            return []\n",
    "        return []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NCF(\n",
      "  (user_embedding_mlp): Embedding(6042, 32)\n",
      "  (item_embedding_mlp): Embedding(3954, 32)\n",
      "  (user_embedding_mf): Embedding(6042, 32)\n",
      "  (item_embedding_mf): Embedding(3954, 32)\n",
      "  (user_features): Sequential(\n",
      "    (0): Linear(in_features=48, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (item_features): Sequential(\n",
      "    (0): Linear(in_features=19, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (MLP): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.1, inplace=False)\n",
      "    (6): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Dropout(p=0.1, inplace=False)\n",
      "    (9): Linear(in_features=128, out_features=32, bias=True)\n",
      "    (10): ReLU()\n",
      "  )\n",
      "  (neu_mf): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      "  (criterion): BCELoss()\n",
      ") \n",
      "Running on:  cpu\n",
      "Number of parameters: 882,785\n",
      "Epoch 1/1\n",
      "450/450 - loss: 0.2494 - Val Loss: 0.2119 - NDCG: 0.9712 - HR: 0.9896 - ROC-AUC: 0.9161 - lr: 0.001\n",
      "Model saved to saved_models\\NCF_final.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing training data:   0%|          | 0/920967 [00:00<?, ?it/s]C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_21388\\2802823159.py:157: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  user_features = torch.tensor([X_users_train[user_id]], dtype=torch.float32)\n",
      "Preparing training data:   1%|          | 10425/920967 [2:05:29<182:41:11,  1.38it/s]    \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 51\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# 2. إنشاء وتدريب النموذج الهجين\u001b[39;00m\n\u001b[0;32m     43\u001b[0m hybrid_rec_lr \u001b[38;5;241m=\u001b[39m HybridRecommenderLR(\n\u001b[0;32m     44\u001b[0m     neu_mf_model\u001b[38;5;241m=\u001b[39mtrained_model,\n\u001b[0;32m     45\u001b[0m     rules\u001b[38;5;241m=\u001b[39mrules,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m     movies_df\u001b[38;5;241m=\u001b[39mmovies_df\n\u001b[0;32m     49\u001b[0m )\n\u001b[1;32m---> 51\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m hybrid_rec_lr\u001b[38;5;241m.\u001b[39mprepare_training_data(y_ratings_train)\n\u001b[0;32m     52\u001b[0m hybrid_rec_lr\u001b[38;5;241m.\u001b[39mtrain_logistic_regression(X_train, y_train)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# 3. التقييم المتقدم\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[27], line 144\u001b[0m, in \u001b[0;36mHybridRecommenderLR.prepare_training_data\u001b[1;34m(self, train_interactions, sample_negatives)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(sample_negatives):\n\u001b[0;32m    143\u001b[0m     neg_item \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(negative_candidates)\n\u001b[1;32m--> 144\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_features(user_id, neg_item)\n\u001b[0;32m    145\u001b[0m     X\u001b[38;5;241m.\u001b[39mappend(features)\n\u001b[0;32m    146\u001b[0m     y\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[1;32mIn[27], line 188\u001b[0m, in \u001b[0;36mHybridRecommenderLR._extract_features\u001b[1;34m(self, user_id, item_id)\u001b[0m\n\u001b[0;32m    185\u001b[0m features\u001b[38;5;241m.\u001b[39mappend(knn_score)\n\u001b[0;32m    187\u001b[0m \u001b[38;5;66;03m# Additional features\u001b[39;00m\n\u001b[1;32m--> 188\u001b[0m item_popularity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mratings_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmovie_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts()\u001b[38;5;241m.\u001b[39mget(item_id, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    189\u001b[0m features\u001b[38;5;241m.\u001b[39mappend(item_popularity)\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m features\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\base.py:1015\u001b[0m, in \u001b[0;36mIndexOpsMixin.value_counts\u001b[1;34m(self, normalize, sort, ascending, bins, dropna)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m    929\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalue_counts\u001b[39m(\n\u001b[0;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    935\u001b[0m     dropna: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    936\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series:\n\u001b[0;32m    937\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    938\u001b[0m \u001b[38;5;124;03m    Return a Series containing counts of unique values.\u001b[39;00m\n\u001b[0;32m    939\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1013\u001b[0m \u001b[38;5;124;03m    Name: count, dtype: int64\u001b[39;00m\n\u001b[0;32m   1014\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1015\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mvalue_counts(\n\u001b[0;32m   1016\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1017\u001b[0m         sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m   1018\u001b[0m         ascending\u001b[38;5;241m=\u001b[39mascending,\n\u001b[0;32m   1019\u001b[0m         normalize\u001b[38;5;241m=\u001b[39mnormalize,\n\u001b[0;32m   1020\u001b[0m         bins\u001b[38;5;241m=\u001b[39mbins,\n\u001b[0;32m   1021\u001b[0m         dropna\u001b[38;5;241m=\u001b[39mdropna,\n\u001b[0;32m   1022\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\algorithms.py:899\u001b[0m, in \u001b[0;36mvalue_counts\u001b[1;34m(values, sort, ascending, normalize, bins, dropna)\u001b[0m\n\u001b[0;32m    896\u001b[0m         result \u001b[38;5;241m=\u001b[39m Series(counts, index\u001b[38;5;241m=\u001b[39midx, name\u001b[38;5;241m=\u001b[39mname, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sort:\n\u001b[1;32m--> 899\u001b[0m     result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39msort_values(ascending\u001b[38;5;241m=\u001b[39mascending)\n\u001b[0;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m normalize:\n\u001b[0;32m    902\u001b[0m     result \u001b[38;5;241m=\u001b[39m result \u001b[38;5;241m/\u001b[39m counts\u001b[38;5;241m.\u001b[39msum()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:3634\u001b[0m, in \u001b[0;36mSeries.sort_values\u001b[1;34m(self, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[0;32m   3632\u001b[0m \u001b[38;5;66;03m# GH 35922. Make sorting stable by leveraging nargsort\u001b[39;00m\n\u001b[0;32m   3633\u001b[0m values_to_sort \u001b[38;5;241m=\u001b[39m ensure_key_mapped(\u001b[38;5;28mself\u001b[39m, key)\u001b[38;5;241m.\u001b[39m_values \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m-> 3634\u001b[0m sorted_index \u001b[38;5;241m=\u001b[39m nargsort(values_to_sort, kind, \u001b[38;5;28mbool\u001b[39m(ascending), na_position)\n\u001b[0;32m   3636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_range_indexer(sorted_index, \u001b[38;5;28mlen\u001b[39m(sorted_index)):\n\u001b[0;32m   3637\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inplace:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\sorting.py:421\u001b[0m, in \u001b[0;36mnargsort\u001b[1;34m(items, kind, ascending, na_position, key, mask)\u001b[0m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    419\u001b[0m     items \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(items)\n\u001b[1;32m--> 421\u001b[0m idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(items))\n\u001b[0;32m    422\u001b[0m non_nans \u001b[38;5;241m=\u001b[39m items[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m    423\u001b[0m non_nan_idx \u001b[38;5;241m=\u001b[39m idx[\u001b[38;5;241m~\u001b[39mmask]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ncf_construction_params = {\n",
    "    'num_users': int(users_data_og['user_id'].max()) + 1,\n",
    "    'num_items': int(items_data_og['movie_id'].max()) + 1,\n",
    "    'user_dim': users_data.shape[1],  # 48\n",
    "    'item_dim': items_data.shape[1],  # 19\n",
    "    'num_factors': 32,  # embedding_dim\n",
    "    'mode': 'implicit',\n",
    "    'criterion': torch.nn.BCELoss(),\n",
    "    'dropout': 0.1,\n",
    "    'lr': 1e-3,\n",
    "    'weight_decay': 1e-5,\n",
    "    'verbose': True,\n",
    "    'gpu': True\n",
    "}\n",
    "\n",
    "# ثانياً: تعريف EarlyStopping و Scheduler\n",
    "early_stopping = EarlyStopping(patience=2, delta=0.001, path='implicit.pth')\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(model.optimizer,\n",
    "                                                       mode='min', factor=0.1, patience=0) # reduces learning rate by factor of 0.1 when no improvement is seen\n",
    "\n",
    "# ثالثاً: التنفيذ الرئيسي\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. تدريب نموذج NeuMF\n",
    "    trainer = ModelTrainer(NCF)\n",
    "    trained_model, history = trainer.train(\n",
    "        train_data=y_ratings_train,\n",
    "        val_data=y_ratings_val,\n",
    "        X_users_train=X_users_train,\n",
    "        X_items_train=X_items_train,\n",
    "        X_users_val=X_users_val,\n",
    "        X_items_val=X_items_val,\n",
    "        epochs=1,\n",
    "        batch_size=2048,\n",
    "        patience=2,\n",
    "        delta=0.001,\n",
    "        **ncf_construction_params\n",
    "    )\n",
    "    \n",
    "    movies_df = items_data_og[['movie_id', 'title', 'genre']].copy()\n",
    "    movies_df.rename(columns={'movie_id': 'movieId', 'genre': 'genres'}, inplace=True)\n",
    "    \n",
    "    # 2. إنشاء وتدريب النموذج الهجين\n",
    "    hybrid_rec_lr = HybridRecommenderLR(\n",
    "        neu_mf_model=trained_model,\n",
    "        rules=rules,\n",
    "        user_item_matrix=user_item_matrix,\n",
    "        ratings_df=ratings_data,\n",
    "        movies_df=movies_df\n",
    "    )\n",
    "    \n",
    "    X_train, y_train = hybrid_rec_lr.prepare_training_data(y_ratings_train)\n",
    "    hybrid_rec_lr.train_logistic_regression(X_train, y_train)\n",
    "    \n",
    "    # 3. التقييم المتقدم\n",
    "    advanced_results = run_advanced_evaluation(\n",
    "        hybrid_rec=hybrid_rec_lr,\n",
    "        y_ratings_train=y_ratings_train,\n",
    "        y_ratings_test=y_ratings_test,\n",
    "        k_values=[5, 10, 20],\n",
    "        sample_size=2000\n",
    "    )\n",
    "    \n",
    "    # 4. عرض النتائج\n",
    "    print(\"\\nFinal Results Summary:\")\n",
    "    print(advanced_results[['model_type', 'k', 'precision@k', 'recall@k', 'ndcg@k', 'map@k', 'diversity', 'novelty']])\n",
    "    \n",
    "    # 5. حفظ النموذج النهائي\n",
    "    torch.save(hybrid_rec_lr, 'final_hybrid_model.pth')\n",
    "    print(\"\\nFinal hybrid model saved to final_hybrid_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
